{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import streamlit as st\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import retrieval_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Development of Multiple Combined\\nRegression Methods for Rainfall\\nMeasurement\\nNusrat Jahan Prottasha\\nDaffodil International University Dhaka 1207, Bangladesh\\nMd. Jashim Uddin\\nNoakhali Science and Technology University, 3814, Dhaka\\nMd. Kowsher\\nStevens Institute of Technology, Hoboken, NJ 07030 USA\\nRokeya Khatun Shorna\\nDaffodil International University, 1207, Dhaka\\nNiaz Al Murshed\\nJahangirnagar University, 1342, Dhaka\\nBoktiar Ahmed Bappy\\nJhenaidah polytechnic institute, 7300, Dhaka\\nCorresponding author: Nusrat Jahan Prottasha, Email: jahannusratprotta@gmail.com\\nRainfallforecastisimperativeasoverwhelmingprecipitationcanleadtonumerous\\ncatastrophes. The prediction makes a difference for individuals to require preven-\\ntivemeasures.In addition,theexpectation oughttobe precise.Most ofthenations\\nin the world is an agricultural nation and most of the economy of any nation de-\\npends upon agriculture. Rain plays an imperative part in agribusiness so the early\\nexpectation of rainfall plays a vital part within the economy of any agricultural.\\n2021. In Raju Pal & Praveen K. Shukla (eds.), SCRS Conference Proceedings\\non Intelligent Systems , 79‚Äì95. Computing & Intelligent Systems, SCRS, India.\\nhttps://doi.org/10.52458/978-93-91842-08-6-7', metadata={'source': 'data\\\\paper.pdf', 'page': 0}), Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nOverwhelming precipitation may well be a major disadvantage. It‚Äôs a cause for\\nnatural disasters like floods and drought that unit of measurement experienced\\nby people over the world each year. Rainfall forecast has been one of the foremost\\nchallengingissuesaroundtheworldinthefinalyear.Therearesomanytechniques\\nthat have been invented for predicting rainfall but most of them are classification,\\nclusteringtechniques.Predictingthequantityofrainpredictioniscrucialforcoun-\\ntries‚Äô people. In our paperwork, we have proposed some regression analysis tech-\\nniques which can be utilized for predicting the quantity of rainfall (The amount of\\nrainfall recorded for the day in mm) based on some historical weather conditions\\ndataset. we have applied 10 supervised regressors (Machine Learning Model) and\\nsome preprocessing methodology to the dataset. We have also analyzed the result\\nandcomparedthemusingvariousstatisticalparametersamongthesetrainedmod-\\nels to find the bestperformed model. Using this model for predicting the quantity\\nof rainfall in some different places. Finally, the Random Forest regressor has pre-\\ndicted the best r2 score of 0.869904217, and the mean absolute error is 0.194459262,\\nmean squared error is 0.126358647 and the root mean squared error is 0.355469615.\\nKeywords: Rainfall, Supervised Learning, Regression, Random Forest Tree, Ad-\\naBoost Regressor, Gradient Boosting Regressor, XGBoos\\n1 Introduction\\nThis research paper proposed a scientific method to predict rainfall quantity\\nbasedonsomedifferentweatherconditionsconsideringprecedingweatherrecords\\nand present weather situations using some regression analysis techniques [1].\\nRainfalldeterminingisexceptionallyvitalsinceoverwhelmingandirregularrain-\\nfall can have numerous impacts on many other things like annihilation of river-\\nbank, crops, agriculture, and farms. One of the very deleterious departures is\\nfloodingduetotheoverrain.AccordingtoWikipediainlatesummer2002,enor-\\nmous storm downpours driven to gigantic flooding in eastern India, Nepal, and\\nBangladesh, killing over 500 individuals and clearing out millions of houses [2].\\nEach year in Bangladesh approximately 26,000 square kilometers (10,000 sq mi)\\n(around 18% of the country) is flooded, killing over 5,000 individuals and wreck-\\ning more than 7 million homes. On the other hand, Western Sydney is now the\\n‚Äùgreatestconcern‚ÄùfromtheworstfloodsindecadestohaveravagedeasternAus-\\ntralia. Rodda et al. [3] presented a very rational method of the rainfall measure-\\nment problem. The application of science and innovation that predicts the state\\noftheenvironmentatanygivenspecificperiodisknownasclimatedetermining\\norweatherforecasting.Therearemanydistinctivestrategiesforclimateestimate\\nand weather forecasting. But rainfall prediction is rare. Some of the research has\\nshownsomeclassificationmethodtopredictwhetheritwouldberaintomorrow\\n80', metadata={'source': 'data\\\\paper.pdf', 'page': 1}), Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nor not. But instead of a classification method for predicting rain, we need to the\\nquantityoftherainfallinaparticularplace.Thereisnumerousequipmentimple-\\nment for foreseeing rainfall by utilizing the climate conditions like temperature,\\nhumidity,weight. These conventional strategies cannot workproductivelyso by\\nutilizing machine learning procedures. we can create an exact comes about rain\\nforecast. Ready to fair do it by having the historical information investigation of\\nrainfall and can anticipate the precipitation for future seasons.\\nIn our paper, we presented some predictive regression analysis techniques to\\nquantify rainfall quantity at a place. Here we used more than 10 years of histor-\\nical data to train our model. The dataset contains various weather conditions of\\ndifferent places. This method can be utilized to predict the rainfall (The amount\\nof rainfall recorded for the day in mm) and avoid the annihilation caused by it to\\nlife, agriculture, farm, and property. If we can quantify the rainfall most people\\ncan make some decisions before overwhelmed rain-affected. The contributions\\nof this work are summarised as:\\n‚Ä¢We have assessed a pipeline of making choices for evaluating the finest\\nreasonable rain prediction.\\n‚Ä¢We have utilized 10 supervised regressors (Machine Learning Model). Be-\\ncause different regressors give us different results. So, it‚Äôs essential to find\\nout the right model according to the requirements.\\n‚Ä¢We have discussed a big comparison among all trained models to figure\\nout the best performer.\\nThe paper is organized as takes after: Section II clarifies the related work of\\ndifferentclassificationstrategiesfortheforecastofrainclassification.Section-III\\ndepictsthetechniqueandmaterialsutilized.Section-IVdepictstheexperimental\\nanalysis including performance and result. Section V talks about the conclusion\\nof this research work where section VI described about the plan of future.\\n2 Related Works\\nIn this paper, through a systematic investigation Rodda et al. [3] have presented\\nthe rainfall measurement problem, they claim there‚Äôs an orderly mistake in the\\nestimation of precipitation made in an ordinary way, a mistake which may in-\\nfluence any gauges utilizing these estimations. Besides Prabakaran et al. [4] pro-\\nposed a method that speaks to a numerical strategy called Linear Regression\\n81', metadata={'source': 'data\\\\paper.pdf', 'page': 2}), Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nto anticipate the rainfall in different areas in southern states of India. To im-\\nprovement Wang et al. [5] showed a case study they proposed an application of\\ngeneralized regression neural network (GRNN) model to anticipate yearly pre-\\ncipitation in Zhengzhou . On the other hand, Sethi et al. [6] presented an ex-\\nploitingdataminingtechniquefortheearlypredictionofrainfallcalledmultiple\\nlinear regression (MLR). Sunyoung Lee et al. [7] presented a divide and conquer\\napproach to predict the rainfall based on the locational information only. Also,\\nM Adil et al. [8] developed the Clusterwise Linear Regression (CLR) technique\\nforthepredictionofmonthlyrainfall.Inaddition,MohammedMoulanaetal.[9]\\nrepresentedmachinelearningtechniquestoprecipitationpredictionthepurpose\\nof this project is to offer non-experts simple get to the methods, approaches uti-\\nlized within the division of precipitation forecast and give a comparative think\\naboutamongthedifferentmachinelearningmethods.Ashaetal.[10]proposeda\\nmutual neural classification model for predicting rainfall. Sakthivel et al. [11] de-\\nscribedneuralnetworksandtherapidminer-basedrainpredictionsystem.Naidu\\net al. [12] presented the changes in rainfall patterns in numerous agro-climatic\\nzones using machine learning approaches. Besides, Dinh et al. [13] utilized an\\nLSHADE-PWI-SVM method for the integration of machine learning classifiers\\nconjointly metaheuristic optimization . On the other hand, Malathi et al. [14]\\nshowedaInformationGainbasedFeatureSelectionMethodforWeatherDataset\\nfor the prediction of rainfall. Also, SamsiahSani et al. [15] evaluated many ma-\\nchine learning classifiers based on Malaysian data for rainfall prediction. Ahi-\\njevych et al. [16] presented a random forest (RF) that is utilized to produce 2-h\\nfiguresoftheprobabilityforthestartofmesoscaleconvectiveframeworks(MCS-\\nI). Allen et al. [17] performed property and agribusiness, as well as handfuls of\\nfatalities and Wonders related to extreme electrical storms. Brooks et al. [18] dis-\\nplayed the current dissemination of serious rainstorms as a work of large-scale\\nnatural conditions. Gentine et al. [19] representing uncertain sodden convection\\nin coarse‚Äêscale climate models remains one of the most bottlenecks of current\\nclimate recreations. McPhaden et al. [20] described the participation of the piv-\\notal for agriculture-dependent. Hazell et al. [21] represented to reduce the risk\\nof life and also maintain the agriculture farms in a better way Then, Mollinga et\\nal. [22] elucidates farmers to take early measurements of floods, and manage the\\nwater resources properly. Shah et al. [23] discussed to related this task to predict\\nrain.\\n82', metadata={'source': 'data\\\\paper.pdf', 'page': 3}), Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n3 Methodology\\nTo perform the complete technique, we assume the four significant steps such\\nas data collection, data pre-processing, training model using 10 supervised re-\\ngressors, and execution examination. Within the information collection step, we\\nhaveusedadataset1fromtheKaggleplatformwhichhasbeensplitintotwoparts\\nsuch as the training part and validation part. Here we have utilized one of the\\nvalidation parts as the testing data to evaluate the models‚Äô performance. Each\\nrow has various weights for decision making to suggest the sensible best rain\\nprediction. Afterward, gathering all raw data, firstly we would be made ready\\nfor the training model with the help of data pre-processing techniques and this\\nhas been used for outliers free and more rigid. It also assists to increase the per-\\nformanceofthemodels.Asaresult,wehaveappliedsixpre-processingmethods\\nsuch as cleaning data, missing value check, handling the categorical data, han-\\ndling outliers, handling outliers, feature selection. Next, to establish supervised\\nregressors models, we utilized the regressors such as Linear Regression, Ridge\\nRegression, Polynomial Regression, and Lasso Regression. From all the training\\nmethods we have used a total of 10 regressors so that we can compare the per-\\nformance and figure out the best model. Most of the regressors come up with a\\ngood performance. We have described the whole methodology in Figure 1.\\nData Collection\\nData Cleaning\\nData Analysis\\nPreprocessingHandling missing value \\nHandling categorical data\\nHandling outliersFeature scalingFeature SelectionSpliting Data\\nTrain Data\\nModel TeÔ¨Ü\\nData\\nTrained\\nModel \\nPrediction\\nFigure 1 :The whole methodology of rainfall prediction including all\\nimportant steps such as data collection, necessary preprocessing, and\\ntraining model with performance prediction\\n3.1 Introduction dataset\\nKindly This dataset contains about 10 years of daily weather observations from\\nmany locations. We have collected this dataset from Kaggle. It is having 23 di-\\n1https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\\n83', metadata={'source': 'data\\\\paper.pdf', 'page': 4}), Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nTable 1:Considering feature‚Äôs description of dataset\\nFeature name Description\\nLocation The common title of the area of the cli-\\nmate station.\\nMinTemp The least temperature in degrees centi-\\ngrade.\\nMaxTemp The most extreme temperature in de-\\ngrees centigrade.\\nRainfall Thesumofprecipitationrecordedforthe\\nday in millimeters.\\nWindGustDir The heading of the most grounded wind\\nblast within 24 h to midnight.\\nWindGustSpeed Thespeed(inkilometersperhour)ofthe\\nstrongest wind blast within 24 h to mid-\\nnight.\\nWindDir9am The course of the wind blast at 9 a.m.\\nWindSpeed9am Wind speed (km/hr) found the middle\\nvalue of over 10 minutes sometime re-\\ncently 9 am.\\nWindSpeed3pm Wind speed (in kilometers per hour)\\nfound the middle value of over 10 min\\nsometime recently 3 p.m.\\nHumidity9am Relative humidity at 9 am.\\nHumidity3pm Relative humidity at 3 pm.\\nPressure 9am Climatic weight (hPa) was decreased to\\ncruel ocean level at 9 a.m.\\nTemp3pm Temperature (degrees C) at 3 p.m.\\nRain Today Numbers1ontheoffchancethatprecipi-\\ntation (in millimeters) within the 24 h to\\n9 a.m. surpasses 1 mm, something else 0.\\n84', metadata={'source': 'data\\\\paper.pdf', 'page': 5}), Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nverseobservationfeaturesofweatherconditionlike‚ÄôLocation‚Äô,‚ÄôMinTemp‚Äô,‚ÄôMax-\\nTemp‚Äô, ‚ÄôRainfall‚Äô, ‚ÄôEvaporation‚Äô, ‚ÄôSunshine‚Äô, ‚ÄôWind Gust Dir‚Äô, ‚ÄôWind Gust Speed‚Äô,\\n‚ÄôWind Dir 9am‚Äô, ‚ÄôWind Dir 3pm‚Äô, ‚ÄôWind Speed 9am‚Äô, ‚ÄôWind Speed 3pm‚Äô, ‚ÄôHumid-\\nity 9am‚Äô, ‚ÄôHumidity 3pm‚Äô, ‚ÄôPressure 9 am‚Äô, ‚ÄôPressure 3pm‚Äô, ‚ÄôCloud 9am‚Äô, ‚ÄôCloud\\n3pm‚Äô, ‚ÄôTemp 9am‚Äô, ‚ÄôTemp 3pm‚Äô, ‚ÄôRain Today‚Äô. Here,in the table 1 the description\\nof the data-set has been illustrated.\\n3.2 Pre-processing\\nInmachinelearning,thedatapreprocessingiswithintheframeworkofexchang-\\ning or encoding the crude information in a stage where calculations can be ef-\\nfectively implemented to prepare. We ought to preprocess the information con-\\ncurringtocreateitfitforthemachinelearningmodel.Well-processeddatagives\\nhigh accuracy and makes the model more solid. Here, we have utilized a few\\nstages of preprocessing strategies, which have been outlined in Figure- 2:\\nInput DataData Cleaning\\nRemoved unused\\nfeature\\nRemoved DuplicatesHandling missing values\\nMean & MedianHandling categorical data\\nEDA\\nOne Hot Encoding\\nHandling outliersIQR\\nMethod\\nFeature ScalingStandard Scaler \\nFeature SelectionNumerical \\nCategorical Model Output\\nFigure 2 :Data Pre-Processing\\nIn our dataset, there are parcels of unused, null, and duplicate values. For this\\nreason, we took some steps to handle these issues. such as,\\n‚Ä¢Erasedduplicaterowandcolumn:wediscoverthatnumerousinformation\\npointsarerepeatedinrowandcolumnsections.Therefore,weexpelledall\\nthe duplicate information.\\n‚Ä¢Erased the row and column, which shows up more than 50% of the null\\nvalue. Cleaning data occurs when 50% of information comes to the null\\nvalue.Atthatpoint,wehavechosentoevacuatethewholerowsandcolumns.\\n85', metadata={'source': 'data\\\\paper.pdf', 'page': 6}), Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nFor the most part, missing value is characterized as the value which was\\nnotputawaywithinthesample.Themissingvaluemaybeacommonocca-\\nsionininformation.Ontheotherhand,mostprescientmodelingstrategies\\ncan‚Äôt handle any missing value. Thus, this issue must be unraveled before\\nmodeling. In some cases, median, mean, mode strategies are utilized to\\noverhaulamissingvalue.Inanycase,theforemostdirectmethodforman-\\nagingthemissingvalueisthemean,median,modestrategy.Herewehave\\nutilized this mean, median & mode strategy for managing missing data\\n‚Ä¢Handled categorical features: Categorical data could be a subjective in-\\nclude whose values are taken on the value of labels. So, we ought to en-\\ncode this sort of information into numbers so that the machine learning\\nmodel can execute scientific operations on it. In our dataset, there exist\\na few categorical features. We have utilized one-hot encoding, one of the\\nforemost prevalent encoding algorithms, to encode the categorical values\\ninto numbers. It is the foremost common approach, and it works well un-\\nless any categorical variable takes a large number of diverse values. After\\nthis encoding, a double matrix is shaped where 1 indicates the presence of\\nany value and 0 indicates the absence of the value.\\n‚Ä¢Inside our dataset, there were a lot of outliers presented: an outlier is a\\nperception point that‚Äôs removed from other perceptions. An outlier may\\nbe due to variations within the estimation or it may appear exploratory\\nmistake the latter are some of the time excluded from the set of informa-\\ntion. An issue of outliers can cause, they tend to be unaffected by littler\\nUI changes that do influence a more whimsical standard population. Bulk\\norders will thrust through littler convenience changes in a way that your\\naverage visitor may not. So to handle the outliers we have used the IQR\\n(interquartile range) method, which is an efficient technique.\\n‚Ä¢Include scaling is one of the significant strategies that are mandatory to\\nstandardize the working data‚Äôs independent features. All things consid-\\nered, there are different strategies like Min-Max Scaling, Variance Scaling,\\nStandardization,MeanNormalization,andUnitvectorsforincludescaling.\\nIn our work, we have applied standard scaling as a feature scaling proce-\\ndure. Here, the exchanged every data point in the range of between -1 and\\n1.\\n86', metadata={'source': 'data\\\\paper.pdf', 'page': 7}), Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n3.3 Training selective models\\nThelinearmodel[24]performswellinmachinelearninglinearly.Weutilizedthe\\nfour regressors as Linear Regression, Ridge Regression, Polynomial Regression,\\nandLassoRegression.Tree-model[25]algorithmsareconsideredtobeoneofthe\\nleading and most utilized supervised learning methods. In this work, we utilize\\na decision tree regressor. We utilized ‚Äùgini‚Äù for the Gini impurity, and the split-\\nter is chosen as ‚Äôbest‚Äô to select the part at each node. Ensemble methods [26]\\nare procedures that make multiple models and combine them to create moves\\nforward. Here, we utilized four ensemble-based regressors. These are Random\\nForest, Gradient Boosting, Adaboost, and XGboost. Afterward, we have utilized\\nthree neighbors regressors of statistical pattern recognition. This is K- nearest\\nneighbors [27],five nearest is chosen for every iteration. Besides, the Manhat-\\ntan distance is chosen for all neighbor classifiers. The support vector machine\\nSVM [28] is used mainly for exploring a hyperplane in ddimensional space that\\nnotably fits a hyperplane in data points. In the linear SVM, weused hinge as loss\\nfunction with l2 penalty.\\nModel \\nLinear Model \\n> Linear Regression\\n> Polynomial Regression\\n> Ridge \\n> LassoTree Model \\n> Decision tree     \\n Regressor NeareÔ¨Ü Neighbor \\n> KNN Regressor Ensemble Model \\n> Random ForeÔ¨Ü\\nRegressor \\n> AdabooÔ¨Ü\\n> Gradient BooÔ¨Üing\\n> XGBooÔ¨Ü\\nSVM\\n> Support V ector \\nMachine\\nFigure 3 :Training Algorithms\\n87', metadata={'source': 'data\\\\paper.pdf', 'page': 8}), Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\n4 Experiment\\nIn the advancement of our test from the proposed work, we have to begin with\\namassedthedemonstrateandpreparedit.10differentregressorsfromsupervised\\nlearning based on distinctive learning techniques have been executed to antici-\\npate precipitation‚Äôs most pertinent mode. This area depicted distinctive test er-\\nrands for the execution investigation and assessment and compared all calcula-\\ntions. Then, we have outlined the test setup utilized to execute the entire errand\\nand utilized 11 statistical assessment measurements for investigation execution.\\nAt long last, we have moreover compared with other works related to this issue\\nconcerning the finest form of our work.\\n4.1 Experiment Setup\\nwe have completed the complete computation in2google collab, a python reen-\\nactment environment given by Google. This environment comes with parallel\\ncomputation facilities for quick execution. We have utilized the foremost well-\\nknownlibrariestocreatesimpleandexpressiveinformationstructuresthatwork\\nwell and instinctively quickly. At long last, sklearn library contains specialized\\nmachine learning and statistical modeling instruments, counting classification,\\nregression,andclusteringcalculationsformodeling.Wehaveutilizedamachine\\nlearning system named3sci-kit learn to implement the regression algorithm.\\nAt long last, we utilized4matplotlib and5seaborn for information visualization,\\ngraphical representation, additionally for information investigation.\\n4.2 Statistical measurement\\nR2score:TheR2scorecouldbeaverycriticalmetricthat‚Äôsutilizedtoassessthe\\nperformance of a regression based machine learning model. It is articulated as\\nR squared and is additionally known as the coefficient of assurance. It works by\\nmeasuring the sum of variance within the expectations clarified by the dataset.\\nBasicallyput,itisthecontrastbetweenthetestswithinthedatasetandtheexpec-\\ntations made by the demonstrate. As we can see from all models Random Forest\\nregressor achieves the best r2 score which is 0.869904217. The second and third\\n2https://colab.research.google.com/\\n3https://scikit-learn.org\\n4https://matplotlib.org\\n5https://seaborn.pydata.org\\n88', metadata={'source': 'data\\\\paper.pdf', 'page': 9}), Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\npositions are achieved by GradientBoostingRegressor and XGBoost which are\\n0.863496747 and 0.863215393. The condition is shown underneath in condition 1:\\nùëÖ2= 1 ‚àí‚àëùëõ\\nùëñ=1(ÃÇ ùë¶ùëñ‚àí ùë¶ùëñ)2\\n‚àëùëõ\\nùëñ=1(ùë¶ùëñ‚àí ÃÑ ùë¶ùëñ)2(7.1)\\nMean absolute error: If we consider with respect to error rate then first comes\\nto mean absolute error. In measurements, mean absolute error may be a degree\\nof blunders between combined perceptions communicating the same wonder.\\nMean Absolute Error (MAE) is another loss function utilized for relapse models.\\nMAE is the entirety of outright contrasts between our target and anticipated fac-\\ntors. So it measures the normal greatness of errors in a set of forecasts, without\\nconsidering their bearings. Random Forest regressor gets the least mean abso-\\nlute error rate which is 0.194459262 compare to others. The declaration of the F1\\nscore is displayed in equation 2 :\\nùëÄùê¥ùê∏ =1\\nùëõùëõ\\n‚àë\\nùëñ=1||ùëåùëñ‚àíÃÇùëåùëñ|| (7.2)\\nMean squared error: If we consider with respect to mean squared error, The\\nmean squared error (MSE) tells how near a relapse line is to a set of focuses. It\\ndoes this by taking the separations from the focuses to the relapse line these sep-\\narationsaretheerrorsandsquaringthem,wecallItmeansquarederror.Fromall\\nthe models Random forest achieves a minimum mean squared error 0.126358647.\\nThe articulation is shown beneath in 3 :\\nùëÄùëÜùê∏ =1\\nùëõùëõ\\n‚àë\\nùëñ=1(ùëåùëñ‚àíÃÇùëåùëñ)2(7.3)\\nRoot mean squared error: Now if we consider the root mean squared error,\\nRoot Mean Square Error (RMSE) means the standard deviation of the residuals\\nwhich is prediction error. Residuals are a degree of how distant from the relapse\\nline information focuses are RMSE could be a degree of how to spread out these\\nresiduals are. Here root mean squared error of Random Forest is 0.355469615\\nwhich is less compare to others. The verbalization is shown in 4:\\nùëÖùëÄùëÜùê∏ =\\n‚àöùëõ\\n‚àë\\nùëñ=1(ÃÇ ùë¶ùëñ‚àí ùë¶ùëñ)2\\nùëõ(7.4)\\n89', metadata={'source': 'data\\\\paper.pdf', 'page': 10}), Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nBy all the statistical performance analysis we can see Random forest is the\\nefficient regressor model and performing well in this use case.\\n4.3 Result & performance analysis\\nTable 2:Performance Metrics of different regressors\\nModel Name r2 score MAE MSE RMSE\\nRandom Forest 0.869904217 0.19445926 0.126358647 0.355469615\\nDecision Tree 0.742284572 0.21508858 0.250312287 0.500312189\\nLinear Regression 0.837495137 0.22694578 0.157836744 0.397286728\\nKNN Regressor 0.401557082 0.48855924 0.581252029 0.762398865\\nAdaBoost Regressor 0.786451397 0.37659111 0.207414199 0.455427491\\nGradient Boosting Regressor 0.863496747 0.20372662 0.132582057 0.364118191\\nXGBoost 0.863215393 0.20367076 0.132855329 0.364493249\\nRidge Regression 0.837495234 0.157836649 0.132855329 0.397286608\\nLasso Regression -5.91E-05 0.83158029 0.971331339 0.985561434\\nSVM 0.841801 0.203451 0.130951 0.345151\\nFrom Table 2, we showed statistical results and comparisons among all ma-\\nchine learning regressors.For better analysis, we choose some statistical pro-\\ncedures for numerical result computing such as r2 score, mean absolute error\\n(MAE), mean square error (MSE), root mean square error (RMSE). After devel-\\noping the models and testing all regressors, We can see that the Random For-\\nest has predicted the best accuracy of 0.869904217 among all others, and the\\nmean absolute error is 0.194459262 which is the lowest, mean squared error is\\n0.126358647 and the root mean squared error is 0.355469615. Considering all er-\\nrors and accuracy, it took the best place. Secondly, the gradient boosting regres-\\nsor has gained better accuracy with the second place which is 0.863496747 with\\nthe mean absolute error is 0.203726623, mean squared error is 0.132582057 and\\nthe root mean squared error is 0.364118191. Thirdly, the XGBoost regressor has\\nacquired better accuracy, which is 0.863215393, along with the mean absolute er-\\n90', metadata={'source': 'data\\\\paper.pdf', 'page': 11}), Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nroris0.203670766,meansquarederroris0.132855329andtherootmeansquared\\nerror is 0.364493249. Also, from the section on linear algorithms, we can figure\\nout that Linear Regression and Ridge Regression showed the almost same accu-\\nracy and so on. So in this analysis, we can although Random forest and Gradient\\nBoosting Regressor have acquired almost the same Accuracy but if we consider\\nthe evaluation metrics of then so, Random forest has a low error rate compare\\nto Gradient Boosting. So, here we have considered the Random forest approach.\\nOverall all of regressors showed a standard and acceptable performance.\\nThe bar chart is a graph for representing all regressors algorithms with Sta-\\ntistical measurement. The bar can be vertically or horizontally. Here is the bar\\ngraph of our selective algorithms, down below.\\nFigure 4 :Selective algorithms\\n5 Conclusion\\nIn this work, we have presented an initial attempt to determine how much rain\\nwill come when it‚Äôs raining time. In the data collection phase, we adopted real\\ndata from Australia from the Kaggle platform. The primary purpose of this task\\nistofindoutthebestregressiontechniqueforthepredictionofrain.Forthisrea-\\nson,wehaveusedavarietyofregressionanalysistechniquesthatcanbeutilized\\nfor predicting the quantity of rainfall so that anyone can use the best predictive\\nmodel in real-life applications. To perform this task, we selected five significant\\n91', metadata={'source': 'data\\\\paper.pdf', 'page': 12}), Document(page_content='steps, these are data collection, data prepossessing, training model using regres-\\nsion analysis techniques, and performance analysis. In pre-processing part, we\\nhave described cleaning data, Missing value check, EDA, Handling outliers, Fea-\\nture selection, Feature scaling respectively. Besides, we used ten supervised re-\\ngressors (machine learning models) for predicting rainfall. Among all models\\nthe are gives good accuracy in our predicting regression. Here, in the figure 4\\nthe graphical performance including compassion among all trained models has\\nbeen depicted\\n6 Future Work\\nInfuturework,wewillfocusonthereal-lifeapplicationofrainfallprediction,so\\nthat anyone especially farmer can use it easily and forecast the weather of rain.\\nAlso, we have plan to use the neural network based deep hybrid approaches\\nto improve the performance. Undoubtedly, we have plans to evaluate the other\\ncountry‚Äôs data for forecasting the rain.', metadata={'source': 'data\\\\paper.pdf', 'page': 13}), Document(page_content='Bibliography\\n[1]Ortiz-Garc√≠a, E. G., S. Salcedo-Sanz, and C. Casanova-Mateo. Accurate pre-\\ncipitation prediction with support vector classifiers: A study including\\nnovel predictive variables and observational data. Atmospheric research,\\n139:128‚Äì136, 2014.\\n[2]IanTyrrell. River Dreams: The people and landscape of the Cooks River . New-\\nSouth, 2018.\\n[3]John C Rodda. The rainfall measurement problem. IAHS Publication No ,\\n78:215‚Äì231, 1967.\\n[4]Gujanatti Rudrappa, Nataraj Vijapur, Rajesh Pattar, Ravi Rathod, Rashmi\\nKulkarni,VuduSreeChandana,andSateeshNHosmane. Machinelearning\\nmodels applied for rainfall prediction. REVISTA GEINTEC-GESTAO INOVA-\\nCAO E TECNOLOGIAS , 11(3):179‚Äì187, 2021.\\n[5]Zhi-liang Wang and Hui-hua Sheng. Rainfall prediction using generalized\\nregression neural network: case study zhengzhou. In International confer-\\nence on computational and information sciences ,pages1265‚Äì1268.IEEE,2010.\\n[6]Nikhil Sethi and Kanwal Garg. Exploiting data mining technique for rain-\\nfall prediction. International Journal of Computer Science and Information\\nTechnologies , 5(3):3982‚Äì3984, 2014.\\n[7]Sunyoung Lee, Sungzoon Cho, and Patrick M Wong. Rainfall prediction\\nusing artificial neural networks. journal of geographic information and De-\\ncision Analysis , 2(2):233‚Äì242, 1998.\\n[8]Adil M Bagirov, Arshad Mahmood, and Andrew Barton. Prediction of\\nmonthly rainfall in victoria, australia: Clusterwise linear regression ap-\\nproach. Atmospheric research , 188:20‚Äì29, 2017.\\n[9]Mohammed Moulana, Kolapalli Roshitha, Golla Niharika, and Maturi Siva\\nSai. Prediction of rainfall using machine learning techniques. International\\nJournal of Scientific & Technology Research , 9:3236‚Äì3240, 2020.\\n2021. In Raju Pal & Praveen K. Shukla (eds.), SCRS Conference Proceedings\\non Intelligent Systems , 79‚Äì95. Computing & Intelligent Systems, SCRS, India.\\nhttps://doi.org/10.52458/978-93-91842-08-6-7', metadata={'source': 'data\\\\paper.pdf', 'page': 14}), Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\n[10]P Asha, A Jesudoss, S Prince Mary, KV Sai Sandeep, and K Harsha Vardhan.\\nAn efficient hybrid machine learning classifier for rainfall prediction. In\\nJournal of Physics: Conference Series , volume 1770, page 012012, 2021.\\n[11]S Sakthivel et al. Effective procedure to predict rainfall conditions using\\nhybrid machine learning strategies. Turkish Journal of Computer and Math-\\nematics Education (TURCOMAT) , 12(6):209‚Äì216, 2021.\\n[12]Diwakar Naidu, Babita Majhi, and Surendra Kumar Chandniha. Develop-\\nment of rainfall prediction models using machine learning approaches for\\ndifferent agro-climatic zones. In Handbook of Research on Automated Fea-\\nture Engineering and Advanced Applications in Data Science , pages 72‚Äì94.\\nIGI Global, 2021.\\n[13]Tuan Vu Dinh, Hieu Nguyen, Xuan-Linh Tran, and Nhat-Duc Hoang. Pre-\\ndicting rainfall-induced soil erosion based on a hybridization of adaptive\\ndifferentialevolutionandsupportvectormachineclassification. Mathemat-\\nical Problems in Engineering , 2021.\\n[14]RMalathi andM Manimekalai. Ant colony‚Äìinformation gainbased feature\\nselection method for weather dataset. Annals of the Romanian Society for\\nCell Biology , pages 3838‚Äì3850, 2021.\\n[15]NorSamsiahSani,IsraaShlash,MohammedHassan,AbdulHadi,andMohd\\nAliff.Enhancingmalaysiarainfallpredictionusingclassificationtechniques.\\nJ. Appl. Environ. Biol. Sci , 7(2S):20‚Äì29, 2017.\\n[16]David Ahijevych, James O Pinto, John K Williams, and Matthias Steiner.\\nProbabilistic forecasts of mesoscale convective system initiation using the\\nrandom forest data mining technique. Weather and Forecasting , 31(2):581‚Äì\\n599, 2016.\\n[17]JohnTAllen. Climatechangeandseverethunderstorms. In Oxford research\\nencyclopedia of climate science . 2018.\\n[18]Harold E Brooks. Severe thunderstorms and climate change. Atmospheric\\nResearch , 123:129‚Äì138, 2013.\\n[19]Pierre Gentine, Mike Pritchard, Stephan Rasp, Gael Reinaudi, and Galen\\nYacalis. Could machine learning break the convection parameterization\\ndeadlock? Geophysical Research Letters , 45(11):5742‚Äì5751, 2018.\\n94', metadata={'source': 'data\\\\paper.pdf', 'page': 15}), Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n[20]Michael J Mcphaden, Gary Meyers, K Ando, Y Masumoto, VSN Murty,\\nMRavichandran,FSyamsudin,J√©r√¥meVialard,LianboYu,andWYu. Rama:\\nthe research moored array for african‚Äìasian‚Äìaustralian monsoon analysis\\nand prediction. Bulletin of the American Meteorological Society , 90(4):459‚Äì\\n480, 2009.\\n[21]Peter BR Hazell. The appropriate role of agricultural insurance in develop-\\ning countries. Journal of International Development , 4(6):567‚Äì581, 1992.\\n[22]Peter P Mollinga, Ruth S Meinzen-Dick, and Douglas J Merrey. Politics,\\npluralityandproblemsheds:Astrategicapproachforreformofagricultural\\nwater resources management. Development Policy Review , 25(6):699‚Äì719,\\n2007.\\n[23]Chirag Shah, Chathra Hendahewa, and Roberto Gonz√°lez-Ib√°√±ez. Rain or\\nshine? forecasting search process performance in exploratory search tasks.\\nJournal of the Association for Information Science and Technology ,67(7):1607‚Äì\\n1623, 2016.\\n[24]GarethJames,DanielaWitten,TrevorHastie,andRobertTibshirani. Linear\\nmodelselectionandregularization. In An introduction to statistical learning ,\\npages 225‚Äì288. Springer, 2021.\\n[25]Raksha Agarwal and Niladri Chatterjee. Langresearchlab_nc at cmcl2021\\nshared task: Predicting gaze behaviour using linguistic features and tree\\nregressors. In Proceedings of the Workshop on Cognitive Modeling and Com-\\nputational Linguistics , pages 79‚Äì84, 2021.\\n[26]S Ben√≠tez-Pe√±a, E Carrizosa, V Guerrero, MD Jim√©nez-Gamero, B Mart√≠n-\\nBarrag√°n, and C Molero-R√≠o. On sparse ensemble methods. 2021.\\n[27]KimdeBie,AnaLucic,andHindaHaned. Totrustornottotrustaregressor:\\nEstimating and explaining trustworthiness of regression predictions. arXiv\\npreprint arXiv:2104.06982 , 2021.\\n[28]MauricioGonz√°lez-Palacio,LinaSep√∫lveda-Cano,andRonalMontoya. Sim-\\nplified path loss lognormal shadow fading model versus a support vector\\nmachine-based regressor comparison for determining reception powers in\\nwlan networks. In International Conference on Information Technology &\\nSystems, pages 431‚Äì441. Springer, 2021.\\n95', metadata={'source': 'data\\\\paper.pdf', 'page': 16})]\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFDirectoryLoader(\"data\")\n",
    "documents = loader.load()\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Development of Multiple Combined\\nRegression Methods for Rainfall\\nMeasurement\\nNusrat Jahan Prottasha\\nDaffodil International University Dhaka 1207, Bangladesh\\nMd. Jashim Uddin\\nNoakhali Science and Technology University, 3814, Dhaka\\nMd. Kowsher\\nStevens Institute of Technology, Hoboken, NJ 07030 USA\\nRokeya Khatun Shorna\\nDaffodil International University, 1207, Dhaka\\nNiaz Al Murshed\\nJahangirnagar University, 1342, Dhaka\\nBoktiar Ahmed Bappy\\nJhenaidah polytechnic institute, 7300, Dhaka\\nCorresponding author: Nusrat Jahan Prottasha, Email: jahannusratprotta@gmail.com\\nRainfallforecastisimperativeasoverwhelmingprecipitationcanleadtonumerous\\ncatastrophes. The prediction makes a difference for individuals to require preven-\\ntivemeasures.In addition,theexpectation oughttobe precise.Most ofthenations\\nin the world is an agricultural nation and most of the economy of any nation de-\\npends upon agriculture. Rain plays an imperative part in agribusiness so the early\\nexpectation of rainfall plays a vital part within the economy of any agricultural.\\n2021. In Raju Pal & Praveen K. Shukla (eds.), SCRS Conference Proceedings\\non Intelligent Systems , 79‚Äì95. Computing & Intelligent Systems, SCRS, India.\\nhttps://doi.org/10.52458/978-93-91842-08-6-7', metadata={'source': 'data\\\\paper.pdf', 'page': 0}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nOverwhelming precipitation may well be a major disadvantage. It‚Äôs a cause for\\nnatural disasters like floods and drought that unit of measurement experienced\\nby people over the world each year. Rainfall forecast has been one of the foremost\\nchallengingissuesaroundtheworldinthefinalyear.Therearesomanytechniques\\nthat have been invented for predicting rainfall but most of them are classification,\\nclusteringtechniques.Predictingthequantityofrainpredictioniscrucialforcoun-\\ntries‚Äô people. In our paperwork, we have proposed some regression analysis tech-\\nniques which can be utilized for predicting the quantity of rainfall (The amount of\\nrainfall recorded for the day in mm) based on some historical weather conditions\\ndataset. we have applied 10 supervised regressors (Machine Learning Model) and\\nsome preprocessing methodology to the dataset. We have also analyzed the result\\nandcomparedthemusingvariousstatisticalparametersamongthesetrainedmod-\\nels to find the bestperformed model. Using this model for predicting the quantity\\nof rainfall in some different places. Finally, the Random Forest regressor has pre-\\ndicted the best r2 score of 0.869904217, and the mean absolute error is 0.194459262,\\nmean squared error is 0.126358647 and the root mean squared error is 0.355469615.\\nKeywords: Rainfall, Supervised Learning, Regression, Random Forest Tree, Ad-\\naBoost Regressor, Gradient Boosting Regressor, XGBoos\\n1 Introduction\\nThis research paper proposed a scientific method to predict rainfall quantity\\nbasedonsomedifferentweatherconditionsconsideringprecedingweatherrecords\\nand present weather situations using some regression analysis techniques [1].\\nRainfalldeterminingisexceptionallyvitalsinceoverwhelmingandirregularrain-\\nfall can have numerous impacts on many other things like annihilation of river-\\nbank, crops, agriculture, and farms. One of the very deleterious departures is\\nfloodingduetotheoverrain.AccordingtoWikipediainlatesummer2002,enor-\\nmous storm downpours driven to gigantic flooding in eastern India, Nepal, and\\nBangladesh, killing over 500 individuals and clearing out millions of houses [2].\\nEach year in Bangladesh approximately 26,000 square kilometers (10,000 sq mi)\\n(around 18% of the country) is flooded, killing over 5,000 individuals and wreck-\\ning more than 7 million homes. On the other hand, Western Sydney is now the\\n‚Äùgreatestconcern‚ÄùfromtheworstfloodsindecadestohaveravagedeasternAus-\\ntralia. Rodda et al. [3] presented a very rational method of the rainfall measure-\\nment problem. The application of science and innovation that predicts the state\\noftheenvironmentatanygivenspecificperiodisknownasclimatedetermining\\norweatherforecasting.Therearemanydistinctivestrategiesforclimateestimate\\nand weather forecasting. But rainfall prediction is rare. Some of the research has\\nshownsomeclassificationmethodtopredictwhetheritwouldberaintomorrow\\n80', metadata={'source': 'data\\\\paper.pdf', 'page': 1}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nor not. But instead of a classification method for predicting rain, we need to the\\nquantityoftherainfallinaparticularplace.Thereisnumerousequipmentimple-\\nment for foreseeing rainfall by utilizing the climate conditions like temperature,\\nhumidity,weight. These conventional strategies cannot workproductivelyso by\\nutilizing machine learning procedures. we can create an exact comes about rain\\nforecast. Ready to fair do it by having the historical information investigation of\\nrainfall and can anticipate the precipitation for future seasons.\\nIn our paper, we presented some predictive regression analysis techniques to\\nquantify rainfall quantity at a place. Here we used more than 10 years of histor-\\nical data to train our model. The dataset contains various weather conditions of\\ndifferent places. This method can be utilized to predict the rainfall (The amount\\nof rainfall recorded for the day in mm) and avoid the annihilation caused by it to\\nlife, agriculture, farm, and property. If we can quantify the rainfall most people\\ncan make some decisions before overwhelmed rain-affected. The contributions\\nof this work are summarised as:\\n‚Ä¢We have assessed a pipeline of making choices for evaluating the finest\\nreasonable rain prediction.\\n‚Ä¢We have utilized 10 supervised regressors (Machine Learning Model). Be-\\ncause different regressors give us different results. So, it‚Äôs essential to find\\nout the right model according to the requirements.\\n‚Ä¢We have discussed a big comparison among all trained models to figure\\nout the best performer.\\nThe paper is organized as takes after: Section II clarifies the related work of\\ndifferentclassificationstrategiesfortheforecastofrainclassification.Section-III\\ndepictsthetechniqueandmaterialsutilized.Section-IVdepictstheexperimental\\nanalysis including performance and result. Section V talks about the conclusion\\nof this research work where section VI described about the plan of future.\\n2 Related Works\\nIn this paper, through a systematic investigation Rodda et al. [3] have presented\\nthe rainfall measurement problem, they claim there‚Äôs an orderly mistake in the\\nestimation of precipitation made in an ordinary way, a mistake which may in-\\nfluence any gauges utilizing these estimations. Besides Prabakaran et al. [4] pro-\\nposed a method that speaks to a numerical strategy called Linear Regression\\n81', metadata={'source': 'data\\\\paper.pdf', 'page': 2}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nto anticipate the rainfall in different areas in southern states of India. To im-\\nprovement Wang et al. [5] showed a case study they proposed an application of\\ngeneralized regression neural network (GRNN) model to anticipate yearly pre-\\ncipitation in Zhengzhou . On the other hand, Sethi et al. [6] presented an ex-\\nploitingdataminingtechniquefortheearlypredictionofrainfallcalledmultiple\\nlinear regression (MLR). Sunyoung Lee et al. [7] presented a divide and conquer\\napproach to predict the rainfall based on the locational information only. Also,\\nM Adil et al. [8] developed the Clusterwise Linear Regression (CLR) technique\\nforthepredictionofmonthlyrainfall.Inaddition,MohammedMoulanaetal.[9]\\nrepresentedmachinelearningtechniquestoprecipitationpredictionthepurpose\\nof this project is to offer non-experts simple get to the methods, approaches uti-\\nlized within the division of precipitation forecast and give a comparative think\\naboutamongthedifferentmachinelearningmethods.Ashaetal.[10]proposeda\\nmutual neural classification model for predicting rainfall. Sakthivel et al. [11] de-\\nscribedneuralnetworksandtherapidminer-basedrainpredictionsystem.Naidu\\net al. [12] presented the changes in rainfall patterns in numerous agro-climatic\\nzones using machine learning approaches. Besides, Dinh et al. [13] utilized an\\nLSHADE-PWI-SVM method for the integration of machine learning classifiers\\nconjointly metaheuristic optimization . On the other hand, Malathi et al. [14]\\nshowedaInformationGainbasedFeatureSelectionMethodforWeatherDataset\\nfor the prediction of rainfall. Also, SamsiahSani et al. [15] evaluated many ma-\\nchine learning classifiers based on Malaysian data for rainfall prediction. Ahi-\\njevych et al. [16] presented a random forest (RF) that is utilized to produce 2-h\\nfiguresoftheprobabilityforthestartofmesoscaleconvectiveframeworks(MCS-\\nI). Allen et al. [17] performed property and agribusiness, as well as handfuls of\\nfatalities and Wonders related to extreme electrical storms. Brooks et al. [18] dis-\\nplayed the current dissemination of serious rainstorms as a work of large-scale\\nnatural conditions. Gentine et al. [19] representing uncertain sodden convection\\nin coarse‚Äêscale climate models remains one of the most bottlenecks of current\\nclimate recreations. McPhaden et al. [20] described the participation of the piv-\\notal for agriculture-dependent. Hazell et al. [21] represented to reduce the risk\\nof life and also maintain the agriculture farms in a better way Then, Mollinga et\\nal. [22] elucidates farmers to take early measurements of floods, and manage the\\nwater resources properly. Shah et al. [23] discussed to related this task to predict\\nrain.\\n82', metadata={'source': 'data\\\\paper.pdf', 'page': 3}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n3 Methodology\\nTo perform the complete technique, we assume the four significant steps such\\nas data collection, data pre-processing, training model using 10 supervised re-\\ngressors, and execution examination. Within the information collection step, we\\nhaveusedadataset1fromtheKaggleplatformwhichhasbeensplitintotwoparts\\nsuch as the training part and validation part. Here we have utilized one of the\\nvalidation parts as the testing data to evaluate the models‚Äô performance. Each\\nrow has various weights for decision making to suggest the sensible best rain\\nprediction. Afterward, gathering all raw data, firstly we would be made ready\\nfor the training model with the help of data pre-processing techniques and this\\nhas been used for outliers free and more rigid. It also assists to increase the per-\\nformanceofthemodels.Asaresult,wehaveappliedsixpre-processingmethods\\nsuch as cleaning data, missing value check, handling the categorical data, han-\\ndling outliers, handling outliers, feature selection. Next, to establish supervised\\nregressors models, we utilized the regressors such as Linear Regression, Ridge\\nRegression, Polynomial Regression, and Lasso Regression. From all the training\\nmethods we have used a total of 10 regressors so that we can compare the per-\\nformance and figure out the best model. Most of the regressors come up with a\\ngood performance. We have described the whole methodology in Figure 1.\\nData Collection\\nData Cleaning\\nData Analysis\\nPreprocessingHandling missing value \\nHandling categorical data\\nHandling outliersFeature scalingFeature SelectionSpliting Data\\nTrain Data\\nModel TeÔ¨Ü\\nData\\nTrained\\nModel \\nPrediction\\nFigure 1 :The whole methodology of rainfall prediction including all\\nimportant steps such as data collection, necessary preprocessing, and\\ntraining model with performance prediction\\n3.1 Introduction dataset\\nKindly This dataset contains about 10 years of daily weather observations from\\nmany locations. We have collected this dataset from Kaggle. It is having 23 di-\\n1https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\\n83', metadata={'source': 'data\\\\paper.pdf', 'page': 4}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nTable 1:Considering feature‚Äôs description of dataset\\nFeature name Description\\nLocation The common title of the area of the cli-\\nmate station.\\nMinTemp The least temperature in degrees centi-\\ngrade.\\nMaxTemp The most extreme temperature in de-\\ngrees centigrade.\\nRainfall Thesumofprecipitationrecordedforthe\\nday in millimeters.\\nWindGustDir The heading of the most grounded wind\\nblast within 24 h to midnight.\\nWindGustSpeed Thespeed(inkilometersperhour)ofthe\\nstrongest wind blast within 24 h to mid-\\nnight.\\nWindDir9am The course of the wind blast at 9 a.m.\\nWindSpeed9am Wind speed (km/hr) found the middle\\nvalue of over 10 minutes sometime re-\\ncently 9 am.\\nWindSpeed3pm Wind speed (in kilometers per hour)\\nfound the middle value of over 10 min\\nsometime recently 3 p.m.\\nHumidity9am Relative humidity at 9 am.\\nHumidity3pm Relative humidity at 3 pm.\\nPressure 9am Climatic weight (hPa) was decreased to\\ncruel ocean level at 9 a.m.\\nTemp3pm Temperature (degrees C) at 3 p.m.\\nRain Today Numbers1ontheoffchancethatprecipi-\\ntation (in millimeters) within the 24 h to\\n9 a.m. surpasses 1 mm, something else 0.\\n84', metadata={'source': 'data\\\\paper.pdf', 'page': 5}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nverseobservationfeaturesofweatherconditionlike‚ÄôLocation‚Äô,‚ÄôMinTemp‚Äô,‚ÄôMax-\\nTemp‚Äô, ‚ÄôRainfall‚Äô, ‚ÄôEvaporation‚Äô, ‚ÄôSunshine‚Äô, ‚ÄôWind Gust Dir‚Äô, ‚ÄôWind Gust Speed‚Äô,\\n‚ÄôWind Dir 9am‚Äô, ‚ÄôWind Dir 3pm‚Äô, ‚ÄôWind Speed 9am‚Äô, ‚ÄôWind Speed 3pm‚Äô, ‚ÄôHumid-\\nity 9am‚Äô, ‚ÄôHumidity 3pm‚Äô, ‚ÄôPressure 9 am‚Äô, ‚ÄôPressure 3pm‚Äô, ‚ÄôCloud 9am‚Äô, ‚ÄôCloud\\n3pm‚Äô, ‚ÄôTemp 9am‚Äô, ‚ÄôTemp 3pm‚Äô, ‚ÄôRain Today‚Äô. Here,in the table 1 the description\\nof the data-set has been illustrated.\\n3.2 Pre-processing\\nInmachinelearning,thedatapreprocessingiswithintheframeworkofexchang-\\ning or encoding the crude information in a stage where calculations can be ef-\\nfectively implemented to prepare. We ought to preprocess the information con-\\ncurringtocreateitfitforthemachinelearningmodel.Well-processeddatagives\\nhigh accuracy and makes the model more solid. Here, we have utilized a few\\nstages of preprocessing strategies, which have been outlined in Figure- 2:\\nInput DataData Cleaning\\nRemoved unused\\nfeature\\nRemoved DuplicatesHandling missing values\\nMean & MedianHandling categorical data\\nEDA\\nOne Hot Encoding\\nHandling outliersIQR\\nMethod\\nFeature ScalingStandard Scaler \\nFeature SelectionNumerical \\nCategorical Model Output\\nFigure 2 :Data Pre-Processing\\nIn our dataset, there are parcels of unused, null, and duplicate values. For this\\nreason, we took some steps to handle these issues. such as,\\n‚Ä¢Erasedduplicaterowandcolumn:wediscoverthatnumerousinformation\\npointsarerepeatedinrowandcolumnsections.Therefore,weexpelledall\\nthe duplicate information.\\n‚Ä¢Erased the row and column, which shows up more than 50% of the null\\nvalue. Cleaning data occurs when 50% of information comes to the null\\nvalue.Atthatpoint,wehavechosentoevacuatethewholerowsandcolumns.\\n85', metadata={'source': 'data\\\\paper.pdf', 'page': 6}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nFor the most part, missing value is characterized as the value which was\\nnotputawaywithinthesample.Themissingvaluemaybeacommonocca-\\nsionininformation.Ontheotherhand,mostprescientmodelingstrategies\\ncan‚Äôt handle any missing value. Thus, this issue must be unraveled before\\nmodeling. In some cases, median, mean, mode strategies are utilized to\\noverhaulamissingvalue.Inanycase,theforemostdirectmethodforman-\\nagingthemissingvalueisthemean,median,modestrategy.Herewehave\\nutilized this mean, median & mode strategy for managing missing data\\n‚Ä¢Handled categorical features: Categorical data could be a subjective in-\\nclude whose values are taken on the value of labels. So, we ought to en-\\ncode this sort of information into numbers so that the machine learning\\nmodel can execute scientific operations on it. In our dataset, there exist\\na few categorical features. We have utilized one-hot encoding, one of the\\nforemost prevalent encoding algorithms, to encode the categorical values\\ninto numbers. It is the foremost common approach, and it works well un-\\nless any categorical variable takes a large number of diverse values. After\\nthis encoding, a double matrix is shaped where 1 indicates the presence of\\nany value and 0 indicates the absence of the value.\\n‚Ä¢Inside our dataset, there were a lot of outliers presented: an outlier is a\\nperception point that‚Äôs removed from other perceptions. An outlier may\\nbe due to variations within the estimation or it may appear exploratory\\nmistake the latter are some of the time excluded from the set of informa-\\ntion. An issue of outliers can cause, they tend to be unaffected by littler\\nUI changes that do influence a more whimsical standard population. Bulk\\norders will thrust through littler convenience changes in a way that your\\naverage visitor may not. So to handle the outliers we have used the IQR\\n(interquartile range) method, which is an efficient technique.\\n‚Ä¢Include scaling is one of the significant strategies that are mandatory to\\nstandardize the working data‚Äôs independent features. All things consid-\\nered, there are different strategies like Min-Max Scaling, Variance Scaling,\\nStandardization,MeanNormalization,andUnitvectorsforincludescaling.\\nIn our work, we have applied standard scaling as a feature scaling proce-\\ndure. Here, the exchanged every data point in the range of between -1 and\\n1.\\n86', metadata={'source': 'data\\\\paper.pdf', 'page': 7}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n3.3 Training selective models\\nThelinearmodel[24]performswellinmachinelearninglinearly.Weutilizedthe\\nfour regressors as Linear Regression, Ridge Regression, Polynomial Regression,\\nandLassoRegression.Tree-model[25]algorithmsareconsideredtobeoneofthe\\nleading and most utilized supervised learning methods. In this work, we utilize\\na decision tree regressor. We utilized ‚Äùgini‚Äù for the Gini impurity, and the split-\\nter is chosen as ‚Äôbest‚Äô to select the part at each node. Ensemble methods [26]\\nare procedures that make multiple models and combine them to create moves\\nforward. Here, we utilized four ensemble-based regressors. These are Random\\nForest, Gradient Boosting, Adaboost, and XGboost. Afterward, we have utilized\\nthree neighbors regressors of statistical pattern recognition. This is K- nearest\\nneighbors [27],five nearest is chosen for every iteration. Besides, the Manhat-\\ntan distance is chosen for all neighbor classifiers. The support vector machine\\nSVM [28] is used mainly for exploring a hyperplane in ddimensional space that\\nnotably fits a hyperplane in data points. In the linear SVM, weused hinge as loss\\nfunction with l2 penalty.\\nModel \\nLinear Model \\n> Linear Regression\\n> Polynomial Regression\\n> Ridge \\n> LassoTree Model \\n> Decision tree     \\n Regressor NeareÔ¨Ü Neighbor \\n> KNN Regressor Ensemble Model \\n> Random ForeÔ¨Ü\\nRegressor \\n> AdabooÔ¨Ü\\n> Gradient BooÔ¨Üing\\n> XGBooÔ¨Ü\\nSVM\\n> Support V ector \\nMachine\\nFigure 3 :Training Algorithms\\n87', metadata={'source': 'data\\\\paper.pdf', 'page': 8}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\n4 Experiment\\nIn the advancement of our test from the proposed work, we have to begin with\\namassedthedemonstrateandpreparedit.10differentregressorsfromsupervised\\nlearning based on distinctive learning techniques have been executed to antici-\\npate precipitation‚Äôs most pertinent mode. This area depicted distinctive test er-\\nrands for the execution investigation and assessment and compared all calcula-\\ntions. Then, we have outlined the test setup utilized to execute the entire errand\\nand utilized 11 statistical assessment measurements for investigation execution.\\nAt long last, we have moreover compared with other works related to this issue\\nconcerning the finest form of our work.\\n4.1 Experiment Setup\\nwe have completed the complete computation in2google collab, a python reen-\\nactment environment given by Google. This environment comes with parallel\\ncomputation facilities for quick execution. We have utilized the foremost well-\\nknownlibrariestocreatesimpleandexpressiveinformationstructuresthatwork\\nwell and instinctively quickly. At long last, sklearn library contains specialized\\nmachine learning and statistical modeling instruments, counting classification,\\nregression,andclusteringcalculationsformodeling.Wehaveutilizedamachine\\nlearning system named3sci-kit learn to implement the regression algorithm.\\nAt long last, we utilized4matplotlib and5seaborn for information visualization,\\ngraphical representation, additionally for information investigation.\\n4.2 Statistical measurement\\nR2score:TheR2scorecouldbeaverycriticalmetricthat‚Äôsutilizedtoassessthe\\nperformance of a regression based machine learning model. It is articulated as\\nR squared and is additionally known as the coefficient of assurance. It works by\\nmeasuring the sum of variance within the expectations clarified by the dataset.\\nBasicallyput,itisthecontrastbetweenthetestswithinthedatasetandtheexpec-\\ntations made by the demonstrate. As we can see from all models Random Forest\\nregressor achieves the best r2 score which is 0.869904217. The second and third\\n2https://colab.research.google.com/\\n3https://scikit-learn.org\\n4https://matplotlib.org\\n5https://seaborn.pydata.org\\n88', metadata={'source': 'data\\\\paper.pdf', 'page': 9}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\npositions are achieved by GradientBoostingRegressor and XGBoost which are\\n0.863496747 and 0.863215393. The condition is shown underneath in condition 1:\\nùëÖ2= 1 ‚àí‚àëùëõ\\nùëñ=1(ÃÇ ùë¶ùëñ‚àí ùë¶ùëñ)2\\n‚àëùëõ\\nùëñ=1(ùë¶ùëñ‚àí ÃÑ ùë¶ùëñ)2(7.1)\\nMean absolute error: If we consider with respect to error rate then first comes\\nto mean absolute error. In measurements, mean absolute error may be a degree\\nof blunders between combined perceptions communicating the same wonder.\\nMean Absolute Error (MAE) is another loss function utilized for relapse models.\\nMAE is the entirety of outright contrasts between our target and anticipated fac-\\ntors. So it measures the normal greatness of errors in a set of forecasts, without\\nconsidering their bearings. Random Forest regressor gets the least mean abso-\\nlute error rate which is 0.194459262 compare to others. The declaration of the F1\\nscore is displayed in equation 2 :\\nùëÄùê¥ùê∏ =1\\nùëõùëõ\\n‚àë\\nùëñ=1||ùëåùëñ‚àíÃÇùëåùëñ|| (7.2)\\nMean squared error: If we consider with respect to mean squared error, The\\nmean squared error (MSE) tells how near a relapse line is to a set of focuses. It\\ndoes this by taking the separations from the focuses to the relapse line these sep-\\narationsaretheerrorsandsquaringthem,wecallItmeansquarederror.Fromall\\nthe models Random forest achieves a minimum mean squared error 0.126358647.\\nThe articulation is shown beneath in 3 :\\nùëÄùëÜùê∏ =1\\nùëõùëõ\\n‚àë\\nùëñ=1(ùëåùëñ‚àíÃÇùëåùëñ)2(7.3)\\nRoot mean squared error: Now if we consider the root mean squared error,\\nRoot Mean Square Error (RMSE) means the standard deviation of the residuals\\nwhich is prediction error. Residuals are a degree of how distant from the relapse\\nline information focuses are RMSE could be a degree of how to spread out these\\nresiduals are. Here root mean squared error of Random Forest is 0.355469615\\nwhich is less compare to others. The verbalization is shown in 4:\\nùëÖùëÄùëÜùê∏ =\\n‚àöùëõ\\n‚àë\\nùëñ=1(ÃÇ ùë¶ùëñ‚àí ùë¶ùëñ)2\\nùëõ(7.4)\\n89', metadata={'source': 'data\\\\paper.pdf', 'page': 10}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nBy all the statistical performance analysis we can see Random forest is the\\nefficient regressor model and performing well in this use case.\\n4.3 Result & performance analysis\\nTable 2:Performance Metrics of different regressors\\nModel Name r2 score MAE MSE RMSE\\nRandom Forest 0.869904217 0.19445926 0.126358647 0.355469615\\nDecision Tree 0.742284572 0.21508858 0.250312287 0.500312189\\nLinear Regression 0.837495137 0.22694578 0.157836744 0.397286728\\nKNN Regressor 0.401557082 0.48855924 0.581252029 0.762398865\\nAdaBoost Regressor 0.786451397 0.37659111 0.207414199 0.455427491\\nGradient Boosting Regressor 0.863496747 0.20372662 0.132582057 0.364118191\\nXGBoost 0.863215393 0.20367076 0.132855329 0.364493249\\nRidge Regression 0.837495234 0.157836649 0.132855329 0.397286608\\nLasso Regression -5.91E-05 0.83158029 0.971331339 0.985561434\\nSVM 0.841801 0.203451 0.130951 0.345151\\nFrom Table 2, we showed statistical results and comparisons among all ma-\\nchine learning regressors.For better analysis, we choose some statistical pro-\\ncedures for numerical result computing such as r2 score, mean absolute error\\n(MAE), mean square error (MSE), root mean square error (RMSE). After devel-\\noping the models and testing all regressors, We can see that the Random For-\\nest has predicted the best accuracy of 0.869904217 among all others, and the\\nmean absolute error is 0.194459262 which is the lowest, mean squared error is\\n0.126358647 and the root mean squared error is 0.355469615. Considering all er-\\nrors and accuracy, it took the best place. Secondly, the gradient boosting regres-\\nsor has gained better accuracy with the second place which is 0.863496747 with\\nthe mean absolute error is 0.203726623, mean squared error is 0.132582057 and\\nthe root mean squared error is 0.364118191. Thirdly, the XGBoost regressor has\\nacquired better accuracy, which is 0.863215393, along with the mean absolute er-\\n90', metadata={'source': 'data\\\\paper.pdf', 'page': 11}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nroris0.203670766,meansquarederroris0.132855329andtherootmeansquared\\nerror is 0.364493249. Also, from the section on linear algorithms, we can figure\\nout that Linear Regression and Ridge Regression showed the almost same accu-\\nracy and so on. So in this analysis, we can although Random forest and Gradient\\nBoosting Regressor have acquired almost the same Accuracy but if we consider\\nthe evaluation metrics of then so, Random forest has a low error rate compare\\nto Gradient Boosting. So, here we have considered the Random forest approach.\\nOverall all of regressors showed a standard and acceptable performance.\\nThe bar chart is a graph for representing all regressors algorithms with Sta-\\ntistical measurement. The bar can be vertically or horizontally. Here is the bar\\ngraph of our selective algorithms, down below.\\nFigure 4 :Selective algorithms\\n5 Conclusion\\nIn this work, we have presented an initial attempt to determine how much rain\\nwill come when it‚Äôs raining time. In the data collection phase, we adopted real\\ndata from Australia from the Kaggle platform. The primary purpose of this task\\nistofindoutthebestregressiontechniqueforthepredictionofrain.Forthisrea-\\nson,wehaveusedavarietyofregressionanalysistechniquesthatcanbeutilized\\nfor predicting the quantity of rainfall so that anyone can use the best predictive\\nmodel in real-life applications. To perform this task, we selected five significant\\n91', metadata={'source': 'data\\\\paper.pdf', 'page': 12}),\n",
       " Document(page_content='steps, these are data collection, data prepossessing, training model using regres-\\nsion analysis techniques, and performance analysis. In pre-processing part, we\\nhave described cleaning data, Missing value check, EDA, Handling outliers, Fea-\\nture selection, Feature scaling respectively. Besides, we used ten supervised re-\\ngressors (machine learning models) for predicting rainfall. Among all models\\nthe are gives good accuracy in our predicting regression. Here, in the figure 4\\nthe graphical performance including compassion among all trained models has\\nbeen depicted\\n6 Future Work\\nInfuturework,wewillfocusonthereal-lifeapplicationofrainfallprediction,so\\nthat anyone especially farmer can use it easily and forecast the weather of rain.\\nAlso, we have plan to use the neural network based deep hybrid approaches\\nto improve the performance. Undoubtedly, we have plans to evaluate the other\\ncountry‚Äôs data for forecasting the rain.', metadata={'source': 'data\\\\paper.pdf', 'page': 13}),\n",
       " Document(page_content='Bibliography\\n[1]Ortiz-Garc√≠a, E. G., S. Salcedo-Sanz, and C. Casanova-Mateo. Accurate pre-\\ncipitation prediction with support vector classifiers: A study including\\nnovel predictive variables and observational data. Atmospheric research,\\n139:128‚Äì136, 2014.\\n[2]IanTyrrell. River Dreams: The people and landscape of the Cooks River . New-\\nSouth, 2018.\\n[3]John C Rodda. The rainfall measurement problem. IAHS Publication No ,\\n78:215‚Äì231, 1967.\\n[4]Gujanatti Rudrappa, Nataraj Vijapur, Rajesh Pattar, Ravi Rathod, Rashmi\\nKulkarni,VuduSreeChandana,andSateeshNHosmane. Machinelearning\\nmodels applied for rainfall prediction. REVISTA GEINTEC-GESTAO INOVA-\\nCAO E TECNOLOGIAS , 11(3):179‚Äì187, 2021.\\n[5]Zhi-liang Wang and Hui-hua Sheng. Rainfall prediction using generalized\\nregression neural network: case study zhengzhou. In International confer-\\nence on computational and information sciences ,pages1265‚Äì1268.IEEE,2010.\\n[6]Nikhil Sethi and Kanwal Garg. Exploiting data mining technique for rain-\\nfall prediction. International Journal of Computer Science and Information\\nTechnologies , 5(3):3982‚Äì3984, 2014.\\n[7]Sunyoung Lee, Sungzoon Cho, and Patrick M Wong. Rainfall prediction\\nusing artificial neural networks. journal of geographic information and De-\\ncision Analysis , 2(2):233‚Äì242, 1998.\\n[8]Adil M Bagirov, Arshad Mahmood, and Andrew Barton. Prediction of\\nmonthly rainfall in victoria, australia: Clusterwise linear regression ap-\\nproach. Atmospheric research , 188:20‚Äì29, 2017.\\n[9]Mohammed Moulana, Kolapalli Roshitha, Golla Niharika, and Maturi Siva\\nSai. Prediction of rainfall using machine learning techniques. International\\nJournal of Scientific & Technology Research , 9:3236‚Äì3240, 2020.\\n2021. In Raju Pal & Praveen K. Shukla (eds.), SCRS Conference Proceedings\\non Intelligent Systems , 79‚Äì95. Computing & Intelligent Systems, SCRS, India.\\nhttps://doi.org/10.52458/978-93-91842-08-6-7', metadata={'source': 'data\\\\paper.pdf', 'page': 14}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\n[10]P Asha, A Jesudoss, S Prince Mary, KV Sai Sandeep, and K Harsha Vardhan.\\nAn efficient hybrid machine learning classifier for rainfall prediction. In\\nJournal of Physics: Conference Series , volume 1770, page 012012, 2021.\\n[11]S Sakthivel et al. Effective procedure to predict rainfall conditions using\\nhybrid machine learning strategies. Turkish Journal of Computer and Math-\\nematics Education (TURCOMAT) , 12(6):209‚Äì216, 2021.\\n[12]Diwakar Naidu, Babita Majhi, and Surendra Kumar Chandniha. Develop-\\nment of rainfall prediction models using machine learning approaches for\\ndifferent agro-climatic zones. In Handbook of Research on Automated Fea-\\nture Engineering and Advanced Applications in Data Science , pages 72‚Äì94.\\nIGI Global, 2021.\\n[13]Tuan Vu Dinh, Hieu Nguyen, Xuan-Linh Tran, and Nhat-Duc Hoang. Pre-\\ndicting rainfall-induced soil erosion based on a hybridization of adaptive\\ndifferentialevolutionandsupportvectormachineclassification. Mathemat-\\nical Problems in Engineering , 2021.\\n[14]RMalathi andM Manimekalai. Ant colony‚Äìinformation gainbased feature\\nselection method for weather dataset. Annals of the Romanian Society for\\nCell Biology , pages 3838‚Äì3850, 2021.\\n[15]NorSamsiahSani,IsraaShlash,MohammedHassan,AbdulHadi,andMohd\\nAliff.Enhancingmalaysiarainfallpredictionusingclassificationtechniques.\\nJ. Appl. Environ. Biol. Sci , 7(2S):20‚Äì29, 2017.\\n[16]David Ahijevych, James O Pinto, John K Williams, and Matthias Steiner.\\nProbabilistic forecasts of mesoscale convective system initiation using the\\nrandom forest data mining technique. Weather and Forecasting , 31(2):581‚Äì\\n599, 2016.\\n[17]JohnTAllen. Climatechangeandseverethunderstorms. In Oxford research\\nencyclopedia of climate science . 2018.\\n[18]Harold E Brooks. Severe thunderstorms and climate change. Atmospheric\\nResearch , 123:129‚Äì138, 2013.\\n[19]Pierre Gentine, Mike Pritchard, Stephan Rasp, Gael Reinaudi, and Galen\\nYacalis. Could machine learning break the convection parameterization\\ndeadlock? Geophysical Research Letters , 45(11):5742‚Äì5751, 2018.\\n94', metadata={'source': 'data\\\\paper.pdf', 'page': 15}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n[20]Michael J Mcphaden, Gary Meyers, K Ando, Y Masumoto, VSN Murty,\\nMRavichandran,FSyamsudin,J√©r√¥meVialard,LianboYu,andWYu. Rama:\\nthe research moored array for african‚Äìasian‚Äìaustralian monsoon analysis\\nand prediction. Bulletin of the American Meteorological Society , 90(4):459‚Äì\\n480, 2009.\\n[21]Peter BR Hazell. The appropriate role of agricultural insurance in develop-\\ning countries. Journal of International Development , 4(6):567‚Äì581, 1992.\\n[22]Peter P Mollinga, Ruth S Meinzen-Dick, and Douglas J Merrey. Politics,\\npluralityandproblemsheds:Astrategicapproachforreformofagricultural\\nwater resources management. Development Policy Review , 25(6):699‚Äì719,\\n2007.\\n[23]Chirag Shah, Chathra Hendahewa, and Roberto Gonz√°lez-Ib√°√±ez. Rain or\\nshine? forecasting search process performance in exploratory search tasks.\\nJournal of the Association for Information Science and Technology ,67(7):1607‚Äì\\n1623, 2016.\\n[24]GarethJames,DanielaWitten,TrevorHastie,andRobertTibshirani. Linear\\nmodelselectionandregularization. In An introduction to statistical learning ,\\npages 225‚Äì288. Springer, 2021.\\n[25]Raksha Agarwal and Niladri Chatterjee. Langresearchlab_nc at cmcl2021\\nshared task: Predicting gaze behaviour using linguistic features and tree\\nregressors. In Proceedings of the Workshop on Cognitive Modeling and Com-\\nputational Linguistics , pages 79‚Äì84, 2021.\\n[26]S Ben√≠tez-Pe√±a, E Carrizosa, V Guerrero, MD Jim√©nez-Gamero, B Mart√≠n-\\nBarrag√°n, and C Molero-R√≠o. On sparse ensemble methods. 2021.\\n[27]KimdeBie,AnaLucic,andHindaHaned. Totrustornottotrustaregressor:\\nEstimating and explaining trustworthiness of regression predictions. arXiv\\npreprint arXiv:2104.06982 , 2021.\\n[28]MauricioGonz√°lez-Palacio,LinaSep√∫lveda-Cano,andRonalMontoya. Sim-\\nplified path loss lognormal shadow fading model versus a support vector\\nmachine-based regressor comparison for determining reception powers in\\nwlan networks. In International Conference on Information Technology &\\nSystems, pages 431‚Äì441. Springer, 2021.\\n95', metadata={'source': 'data\\\\paper.pdf', 'page': 16})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Development of Multiple Combined\\nRegression Methods for Rainfall\\nMeasurement\\nNusrat Jahan Prottasha\\nDaffodil International University Dhaka 1207, Bangladesh\\nMd. Jashim Uddin\\nNoakhali Science and Technology University, 3814, Dhaka\\nMd. Kowsher\\nStevens Institute of Technology, Hoboken, NJ 07030 USA\\nRokeya Khatun Shorna\\nDaffodil International University, 1207, Dhaka\\nNiaz Al Murshed\\nJahangirnagar University, 1342, Dhaka\\nBoktiar Ahmed Bappy\\nJhenaidah polytechnic institute, 7300, Dhaka\\nCorresponding author: Nusrat Jahan Prottasha, Email: jahannusratprotta@gmail.com\\nRainfallforecastisimperativeasoverwhelmingprecipitationcanleadtonumerous\\ncatastrophes. The prediction makes a difference for individuals to require preven-\\ntivemeasures.In addition,theexpectation oughttobe precise.Most ofthenations\\nin the world is an agricultural nation and most of the economy of any nation de-\\npends upon agriculture. Rain plays an imperative part in agribusiness so the early', metadata={'source': 'data\\\\paper.pdf', 'page': 0}),\n",
       " Document(page_content='tivemeasures.In addition,theexpectation oughttobe precise.Most ofthenations\\nin the world is an agricultural nation and most of the economy of any nation de-\\npends upon agriculture. Rain plays an imperative part in agribusiness so the early\\nexpectation of rainfall plays a vital part within the economy of any agricultural.\\n2021. In Raju Pal & Praveen K. Shukla (eds.), SCRS Conference Proceedings\\non Intelligent Systems , 79‚Äì95. Computing & Intelligent Systems, SCRS, India.\\nhttps://doi.org/10.52458/978-93-91842-08-6-7', metadata={'source': 'data\\\\paper.pdf', 'page': 0}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nOverwhelming precipitation may well be a major disadvantage. It‚Äôs a cause for\\nnatural disasters like floods and drought that unit of measurement experienced\\nby people over the world each year. Rainfall forecast has been one of the foremost\\nchallengingissuesaroundtheworldinthefinalyear.Therearesomanytechniques\\nthat have been invented for predicting rainfall but most of them are classification,\\nclusteringtechniques.Predictingthequantityofrainpredictioniscrucialforcoun-\\ntries‚Äô people. In our paperwork, we have proposed some regression analysis tech-\\nniques which can be utilized for predicting the quantity of rainfall (The amount of\\nrainfall recorded for the day in mm) based on some historical weather conditions\\ndataset. we have applied 10 supervised regressors (Machine Learning Model) and\\nsome preprocessing methodology to the dataset. We have also analyzed the result', metadata={'source': 'data\\\\paper.pdf', 'page': 1}),\n",
       " Document(page_content='rainfall recorded for the day in mm) based on some historical weather conditions\\ndataset. we have applied 10 supervised regressors (Machine Learning Model) and\\nsome preprocessing methodology to the dataset. We have also analyzed the result\\nandcomparedthemusingvariousstatisticalparametersamongthesetrainedmod-\\nels to find the bestperformed model. Using this model for predicting the quantity\\nof rainfall in some different places. Finally, the Random Forest regressor has pre-\\ndicted the best r2 score of 0.869904217, and the mean absolute error is 0.194459262,\\nmean squared error is 0.126358647 and the root mean squared error is 0.355469615.\\nKeywords: Rainfall, Supervised Learning, Regression, Random Forest Tree, Ad-\\naBoost Regressor, Gradient Boosting Regressor, XGBoos\\n1 Introduction\\nThis research paper proposed a scientific method to predict rainfall quantity\\nbasedonsomedifferentweatherconditionsconsideringprecedingweatherrecords', metadata={'source': 'data\\\\paper.pdf', 'page': 1}),\n",
       " Document(page_content='Keywords: Rainfall, Supervised Learning, Regression, Random Forest Tree, Ad-\\naBoost Regressor, Gradient Boosting Regressor, XGBoos\\n1 Introduction\\nThis research paper proposed a scientific method to predict rainfall quantity\\nbasedonsomedifferentweatherconditionsconsideringprecedingweatherrecords\\nand present weather situations using some regression analysis techniques [1].\\nRainfalldeterminingisexceptionallyvitalsinceoverwhelmingandirregularrain-\\nfall can have numerous impacts on many other things like annihilation of river-\\nbank, crops, agriculture, and farms. One of the very deleterious departures is\\nfloodingduetotheoverrain.AccordingtoWikipediainlatesummer2002,enor-\\nmous storm downpours driven to gigantic flooding in eastern India, Nepal, and\\nBangladesh, killing over 500 individuals and clearing out millions of houses [2].\\nEach year in Bangladesh approximately 26,000 square kilometers (10,000 sq mi)\\n(around 18% of the country) is flooded, killing over 5,000 individuals and wreck-', metadata={'source': 'data\\\\paper.pdf', 'page': 1}),\n",
       " Document(page_content='Bangladesh, killing over 500 individuals and clearing out millions of houses [2].\\nEach year in Bangladesh approximately 26,000 square kilometers (10,000 sq mi)\\n(around 18% of the country) is flooded, killing over 5,000 individuals and wreck-\\ning more than 7 million homes. On the other hand, Western Sydney is now the\\n‚Äùgreatestconcern‚ÄùfromtheworstfloodsindecadestohaveravagedeasternAus-\\ntralia. Rodda et al. [3] presented a very rational method of the rainfall measure-\\nment problem. The application of science and innovation that predicts the state\\noftheenvironmentatanygivenspecificperiodisknownasclimatedetermining\\norweatherforecasting.Therearemanydistinctivestrategiesforclimateestimate\\nand weather forecasting. But rainfall prediction is rare. Some of the research has\\nshownsomeclassificationmethodtopredictwhetheritwouldberaintomorrow\\n80', metadata={'source': 'data\\\\paper.pdf', 'page': 1}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nor not. But instead of a classification method for predicting rain, we need to the\\nquantityoftherainfallinaparticularplace.Thereisnumerousequipmentimple-\\nment for foreseeing rainfall by utilizing the climate conditions like temperature,\\nhumidity,weight. These conventional strategies cannot workproductivelyso by\\nutilizing machine learning procedures. we can create an exact comes about rain\\nforecast. Ready to fair do it by having the historical information investigation of\\nrainfall and can anticipate the precipitation for future seasons.\\nIn our paper, we presented some predictive regression analysis techniques to\\nquantify rainfall quantity at a place. Here we used more than 10 years of histor-\\nical data to train our model. The dataset contains various weather conditions of\\ndifferent places. This method can be utilized to predict the rainfall (The amount', metadata={'source': 'data\\\\paper.pdf', 'page': 2}),\n",
       " Document(page_content='quantify rainfall quantity at a place. Here we used more than 10 years of histor-\\nical data to train our model. The dataset contains various weather conditions of\\ndifferent places. This method can be utilized to predict the rainfall (The amount\\nof rainfall recorded for the day in mm) and avoid the annihilation caused by it to\\nlife, agriculture, farm, and property. If we can quantify the rainfall most people\\ncan make some decisions before overwhelmed rain-affected. The contributions\\nof this work are summarised as:\\n‚Ä¢We have assessed a pipeline of making choices for evaluating the finest\\nreasonable rain prediction.\\n‚Ä¢We have utilized 10 supervised regressors (Machine Learning Model). Be-\\ncause different regressors give us different results. So, it‚Äôs essential to find\\nout the right model according to the requirements.\\n‚Ä¢We have discussed a big comparison among all trained models to figure\\nout the best performer.\\nThe paper is organized as takes after: Section II clarifies the related work of', metadata={'source': 'data\\\\paper.pdf', 'page': 2}),\n",
       " Document(page_content='out the right model according to the requirements.\\n‚Ä¢We have discussed a big comparison among all trained models to figure\\nout the best performer.\\nThe paper is organized as takes after: Section II clarifies the related work of\\ndifferentclassificationstrategiesfortheforecastofrainclassification.Section-III\\ndepictsthetechniqueandmaterialsutilized.Section-IVdepictstheexperimental\\nanalysis including performance and result. Section V talks about the conclusion\\nof this research work where section VI described about the plan of future.\\n2 Related Works\\nIn this paper, through a systematic investigation Rodda et al. [3] have presented\\nthe rainfall measurement problem, they claim there‚Äôs an orderly mistake in the\\nestimation of precipitation made in an ordinary way, a mistake which may in-\\nfluence any gauges utilizing these estimations. Besides Prabakaran et al. [4] pro-\\nposed a method that speaks to a numerical strategy called Linear Regression\\n81', metadata={'source': 'data\\\\paper.pdf', 'page': 2}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nto anticipate the rainfall in different areas in southern states of India. To im-\\nprovement Wang et al. [5] showed a case study they proposed an application of\\ngeneralized regression neural network (GRNN) model to anticipate yearly pre-\\ncipitation in Zhengzhou . On the other hand, Sethi et al. [6] presented an ex-\\nploitingdataminingtechniquefortheearlypredictionofrainfallcalledmultiple\\nlinear regression (MLR). Sunyoung Lee et al. [7] presented a divide and conquer\\napproach to predict the rainfall based on the locational information only. Also,\\nM Adil et al. [8] developed the Clusterwise Linear Regression (CLR) technique\\nforthepredictionofmonthlyrainfall.Inaddition,MohammedMoulanaetal.[9]\\nrepresentedmachinelearningtechniquestoprecipitationpredictionthepurpose\\nof this project is to offer non-experts simple get to the methods, approaches uti-', metadata={'source': 'data\\\\paper.pdf', 'page': 3}),\n",
       " Document(page_content='forthepredictionofmonthlyrainfall.Inaddition,MohammedMoulanaetal.[9]\\nrepresentedmachinelearningtechniquestoprecipitationpredictionthepurpose\\nof this project is to offer non-experts simple get to the methods, approaches uti-\\nlized within the division of precipitation forecast and give a comparative think\\naboutamongthedifferentmachinelearningmethods.Ashaetal.[10]proposeda\\nmutual neural classification model for predicting rainfall. Sakthivel et al. [11] de-\\nscribedneuralnetworksandtherapidminer-basedrainpredictionsystem.Naidu\\net al. [12] presented the changes in rainfall patterns in numerous agro-climatic\\nzones using machine learning approaches. Besides, Dinh et al. [13] utilized an\\nLSHADE-PWI-SVM method for the integration of machine learning classifiers\\nconjointly metaheuristic optimization . On the other hand, Malathi et al. [14]\\nshowedaInformationGainbasedFeatureSelectionMethodforWeatherDataset\\nfor the prediction of rainfall. Also, SamsiahSani et al. [15] evaluated many ma-', metadata={'source': 'data\\\\paper.pdf', 'page': 3}),\n",
       " Document(page_content='conjointly metaheuristic optimization . On the other hand, Malathi et al. [14]\\nshowedaInformationGainbasedFeatureSelectionMethodforWeatherDataset\\nfor the prediction of rainfall. Also, SamsiahSani et al. [15] evaluated many ma-\\nchine learning classifiers based on Malaysian data for rainfall prediction. Ahi-\\njevych et al. [16] presented a random forest (RF) that is utilized to produce 2-h\\nfiguresoftheprobabilityforthestartofmesoscaleconvectiveframeworks(MCS-\\nI). Allen et al. [17] performed property and agribusiness, as well as handfuls of\\nfatalities and Wonders related to extreme electrical storms. Brooks et al. [18] dis-\\nplayed the current dissemination of serious rainstorms as a work of large-scale\\nnatural conditions. Gentine et al. [19] representing uncertain sodden convection\\nin coarse‚Äêscale climate models remains one of the most bottlenecks of current\\nclimate recreations. McPhaden et al. [20] described the participation of the piv-', metadata={'source': 'data\\\\paper.pdf', 'page': 3}),\n",
       " Document(page_content='natural conditions. Gentine et al. [19] representing uncertain sodden convection\\nin coarse‚Äêscale climate models remains one of the most bottlenecks of current\\nclimate recreations. McPhaden et al. [20] described the participation of the piv-\\notal for agriculture-dependent. Hazell et al. [21] represented to reduce the risk\\nof life and also maintain the agriculture farms in a better way Then, Mollinga et\\nal. [22] elucidates farmers to take early measurements of floods, and manage the\\nwater resources properly. Shah et al. [23] discussed to related this task to predict\\nrain.\\n82', metadata={'source': 'data\\\\paper.pdf', 'page': 3}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n3 Methodology\\nTo perform the complete technique, we assume the four significant steps such\\nas data collection, data pre-processing, training model using 10 supervised re-\\ngressors, and execution examination. Within the information collection step, we\\nhaveusedadataset1fromtheKaggleplatformwhichhasbeensplitintotwoparts\\nsuch as the training part and validation part. Here we have utilized one of the\\nvalidation parts as the testing data to evaluate the models‚Äô performance. Each\\nrow has various weights for decision making to suggest the sensible best rain\\nprediction. Afterward, gathering all raw data, firstly we would be made ready\\nfor the training model with the help of data pre-processing techniques and this\\nhas been used for outliers free and more rigid. It also assists to increase the per-\\nformanceofthemodels.Asaresult,wehaveappliedsixpre-processingmethods', metadata={'source': 'data\\\\paper.pdf', 'page': 4}),\n",
       " Document(page_content='for the training model with the help of data pre-processing techniques and this\\nhas been used for outliers free and more rigid. It also assists to increase the per-\\nformanceofthemodels.Asaresult,wehaveappliedsixpre-processingmethods\\nsuch as cleaning data, missing value check, handling the categorical data, han-\\ndling outliers, handling outliers, feature selection. Next, to establish supervised\\nregressors models, we utilized the regressors such as Linear Regression, Ridge\\nRegression, Polynomial Regression, and Lasso Regression. From all the training\\nmethods we have used a total of 10 regressors so that we can compare the per-\\nformance and figure out the best model. Most of the regressors come up with a\\ngood performance. We have described the whole methodology in Figure 1.\\nData Collection\\nData Cleaning\\nData Analysis\\nPreprocessingHandling missing value \\nHandling categorical data\\nHandling outliersFeature scalingFeature SelectionSpliting Data\\nTrain Data\\nModel TeÔ¨Ü\\nData\\nTrained\\nModel', metadata={'source': 'data\\\\paper.pdf', 'page': 4}),\n",
       " Document(page_content='good performance. We have described the whole methodology in Figure 1.\\nData Collection\\nData Cleaning\\nData Analysis\\nPreprocessingHandling missing value \\nHandling categorical data\\nHandling outliersFeature scalingFeature SelectionSpliting Data\\nTrain Data\\nModel TeÔ¨Ü\\nData\\nTrained\\nModel \\nPrediction\\nFigure 1 :The whole methodology of rainfall prediction including all\\nimportant steps such as data collection, necessary preprocessing, and\\ntraining model with performance prediction\\n3.1 Introduction dataset\\nKindly This dataset contains about 10 years of daily weather observations from\\nmany locations. We have collected this dataset from Kaggle. It is having 23 di-\\n1https://www.kaggle.com/jsphyg/weather-dataset-rattle-package\\n83', metadata={'source': 'data\\\\paper.pdf', 'page': 4}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nTable 1:Considering feature‚Äôs description of dataset\\nFeature name Description\\nLocation The common title of the area of the cli-\\nmate station.\\nMinTemp The least temperature in degrees centi-\\ngrade.\\nMaxTemp The most extreme temperature in de-\\ngrees centigrade.\\nRainfall Thesumofprecipitationrecordedforthe\\nday in millimeters.\\nWindGustDir The heading of the most grounded wind\\nblast within 24 h to midnight.\\nWindGustSpeed Thespeed(inkilometersperhour)ofthe\\nstrongest wind blast within 24 h to mid-\\nnight.\\nWindDir9am The course of the wind blast at 9 a.m.\\nWindSpeed9am Wind speed (km/hr) found the middle\\nvalue of over 10 minutes sometime re-\\ncently 9 am.\\nWindSpeed3pm Wind speed (in kilometers per hour)\\nfound the middle value of over 10 min\\nsometime recently 3 p.m.\\nHumidity9am Relative humidity at 9 am.\\nHumidity3pm Relative humidity at 3 pm.', metadata={'source': 'data\\\\paper.pdf', 'page': 5}),\n",
       " Document(page_content='WindSpeed9am Wind speed (km/hr) found the middle\\nvalue of over 10 minutes sometime re-\\ncently 9 am.\\nWindSpeed3pm Wind speed (in kilometers per hour)\\nfound the middle value of over 10 min\\nsometime recently 3 p.m.\\nHumidity9am Relative humidity at 9 am.\\nHumidity3pm Relative humidity at 3 pm.\\nPressure 9am Climatic weight (hPa) was decreased to\\ncruel ocean level at 9 a.m.\\nTemp3pm Temperature (degrees C) at 3 p.m.\\nRain Today Numbers1ontheoffchancethatprecipi-\\ntation (in millimeters) within the 24 h to\\n9 a.m. surpasses 1 mm, something else 0.\\n84', metadata={'source': 'data\\\\paper.pdf', 'page': 5}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nverseobservationfeaturesofweatherconditionlike‚ÄôLocation‚Äô,‚ÄôMinTemp‚Äô,‚ÄôMax-\\nTemp‚Äô, ‚ÄôRainfall‚Äô, ‚ÄôEvaporation‚Äô, ‚ÄôSunshine‚Äô, ‚ÄôWind Gust Dir‚Äô, ‚ÄôWind Gust Speed‚Äô,\\n‚ÄôWind Dir 9am‚Äô, ‚ÄôWind Dir 3pm‚Äô, ‚ÄôWind Speed 9am‚Äô, ‚ÄôWind Speed 3pm‚Äô, ‚ÄôHumid-\\nity 9am‚Äô, ‚ÄôHumidity 3pm‚Äô, ‚ÄôPressure 9 am‚Äô, ‚ÄôPressure 3pm‚Äô, ‚ÄôCloud 9am‚Äô, ‚ÄôCloud\\n3pm‚Äô, ‚ÄôTemp 9am‚Äô, ‚ÄôTemp 3pm‚Äô, ‚ÄôRain Today‚Äô. Here,in the table 1 the description\\nof the data-set has been illustrated.\\n3.2 Pre-processing\\nInmachinelearning,thedatapreprocessingiswithintheframeworkofexchang-\\ning or encoding the crude information in a stage where calculations can be ef-\\nfectively implemented to prepare. We ought to preprocess the information con-\\ncurringtocreateitfitforthemachinelearningmodel.Well-processeddatagives\\nhigh accuracy and makes the model more solid. Here, we have utilized a few\\nstages of preprocessing strategies, which have been outlined in Figure- 2:\\nInput DataData Cleaning\\nRemoved unused\\nfeature', metadata={'source': 'data\\\\paper.pdf', 'page': 6}),\n",
       " Document(page_content='curringtocreateitfitforthemachinelearningmodel.Well-processeddatagives\\nhigh accuracy and makes the model more solid. Here, we have utilized a few\\nstages of preprocessing strategies, which have been outlined in Figure- 2:\\nInput DataData Cleaning\\nRemoved unused\\nfeature\\nRemoved DuplicatesHandling missing values\\nMean & MedianHandling categorical data\\nEDA\\nOne Hot Encoding\\nHandling outliersIQR\\nMethod\\nFeature ScalingStandard Scaler \\nFeature SelectionNumerical \\nCategorical Model Output\\nFigure 2 :Data Pre-Processing\\nIn our dataset, there are parcels of unused, null, and duplicate values. For this\\nreason, we took some steps to handle these issues. such as,\\n‚Ä¢Erasedduplicaterowandcolumn:wediscoverthatnumerousinformation\\npointsarerepeatedinrowandcolumnsections.Therefore,weexpelledall\\nthe duplicate information.\\n‚Ä¢Erased the row and column, which shows up more than 50% of the null\\nvalue. Cleaning data occurs when 50% of information comes to the null', metadata={'source': 'data\\\\paper.pdf', 'page': 6}),\n",
       " Document(page_content='‚Ä¢Erasedduplicaterowandcolumn:wediscoverthatnumerousinformation\\npointsarerepeatedinrowandcolumnsections.Therefore,weexpelledall\\nthe duplicate information.\\n‚Ä¢Erased the row and column, which shows up more than 50% of the null\\nvalue. Cleaning data occurs when 50% of information comes to the null\\nvalue.Atthatpoint,wehavechosentoevacuatethewholerowsandcolumns.\\n85', metadata={'source': 'data\\\\paper.pdf', 'page': 6}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nFor the most part, missing value is characterized as the value which was\\nnotputawaywithinthesample.Themissingvaluemaybeacommonocca-\\nsionininformation.Ontheotherhand,mostprescientmodelingstrategies\\ncan‚Äôt handle any missing value. Thus, this issue must be unraveled before\\nmodeling. In some cases, median, mean, mode strategies are utilized to\\noverhaulamissingvalue.Inanycase,theforemostdirectmethodforman-\\nagingthemissingvalueisthemean,median,modestrategy.Herewehave\\nutilized this mean, median & mode strategy for managing missing data\\n‚Ä¢Handled categorical features: Categorical data could be a subjective in-\\nclude whose values are taken on the value of labels. So, we ought to en-\\ncode this sort of information into numbers so that the machine learning\\nmodel can execute scientific operations on it. In our dataset, there exist', metadata={'source': 'data\\\\paper.pdf', 'page': 7}),\n",
       " Document(page_content='‚Ä¢Handled categorical features: Categorical data could be a subjective in-\\nclude whose values are taken on the value of labels. So, we ought to en-\\ncode this sort of information into numbers so that the machine learning\\nmodel can execute scientific operations on it. In our dataset, there exist\\na few categorical features. We have utilized one-hot encoding, one of the\\nforemost prevalent encoding algorithms, to encode the categorical values\\ninto numbers. It is the foremost common approach, and it works well un-\\nless any categorical variable takes a large number of diverse values. After\\nthis encoding, a double matrix is shaped where 1 indicates the presence of\\nany value and 0 indicates the absence of the value.\\n‚Ä¢Inside our dataset, there were a lot of outliers presented: an outlier is a\\nperception point that‚Äôs removed from other perceptions. An outlier may\\nbe due to variations within the estimation or it may appear exploratory', metadata={'source': 'data\\\\paper.pdf', 'page': 7}),\n",
       " Document(page_content='any value and 0 indicates the absence of the value.\\n‚Ä¢Inside our dataset, there were a lot of outliers presented: an outlier is a\\nperception point that‚Äôs removed from other perceptions. An outlier may\\nbe due to variations within the estimation or it may appear exploratory\\nmistake the latter are some of the time excluded from the set of informa-\\ntion. An issue of outliers can cause, they tend to be unaffected by littler\\nUI changes that do influence a more whimsical standard population. Bulk\\norders will thrust through littler convenience changes in a way that your\\naverage visitor may not. So to handle the outliers we have used the IQR\\n(interquartile range) method, which is an efficient technique.\\n‚Ä¢Include scaling is one of the significant strategies that are mandatory to\\nstandardize the working data‚Äôs independent features. All things consid-\\nered, there are different strategies like Min-Max Scaling, Variance Scaling,\\nStandardization,MeanNormalization,andUnitvectorsforincludescaling.', metadata={'source': 'data\\\\paper.pdf', 'page': 7}),\n",
       " Document(page_content='‚Ä¢Include scaling is one of the significant strategies that are mandatory to\\nstandardize the working data‚Äôs independent features. All things consid-\\nered, there are different strategies like Min-Max Scaling, Variance Scaling,\\nStandardization,MeanNormalization,andUnitvectorsforincludescaling.\\nIn our work, we have applied standard scaling as a feature scaling proce-\\ndure. Here, the exchanged every data point in the range of between -1 and\\n1.\\n86', metadata={'source': 'data\\\\paper.pdf', 'page': 7}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n3.3 Training selective models\\nThelinearmodel[24]performswellinmachinelearninglinearly.Weutilizedthe\\nfour regressors as Linear Regression, Ridge Regression, Polynomial Regression,\\nandLassoRegression.Tree-model[25]algorithmsareconsideredtobeoneofthe\\nleading and most utilized supervised learning methods. In this work, we utilize\\na decision tree regressor. We utilized ‚Äùgini‚Äù for the Gini impurity, and the split-\\nter is chosen as ‚Äôbest‚Äô to select the part at each node. Ensemble methods [26]\\nare procedures that make multiple models and combine them to create moves\\nforward. Here, we utilized four ensemble-based regressors. These are Random\\nForest, Gradient Boosting, Adaboost, and XGboost. Afterward, we have utilized\\nthree neighbors regressors of statistical pattern recognition. This is K- nearest\\nneighbors [27],five nearest is chosen for every iteration. Besides, the Manhat-', metadata={'source': 'data\\\\paper.pdf', 'page': 8}),\n",
       " Document(page_content='Forest, Gradient Boosting, Adaboost, and XGboost. Afterward, we have utilized\\nthree neighbors regressors of statistical pattern recognition. This is K- nearest\\nneighbors [27],five nearest is chosen for every iteration. Besides, the Manhat-\\ntan distance is chosen for all neighbor classifiers. The support vector machine\\nSVM [28] is used mainly for exploring a hyperplane in ddimensional space that\\nnotably fits a hyperplane in data points. In the linear SVM, weused hinge as loss\\nfunction with l2 penalty.\\nModel \\nLinear Model \\n> Linear Regression\\n> Polynomial Regression\\n> Ridge \\n> LassoTree Model \\n> Decision tree     \\n Regressor NeareÔ¨Ü Neighbor \\n> KNN Regressor Ensemble Model \\n> Random ForeÔ¨Ü\\nRegressor \\n> AdabooÔ¨Ü\\n> Gradient BooÔ¨Üing\\n> XGBooÔ¨Ü\\nSVM\\n> Support V ector \\nMachine\\nFigure 3 :Training Algorithms\\n87', metadata={'source': 'data\\\\paper.pdf', 'page': 8}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\n4 Experiment\\nIn the advancement of our test from the proposed work, we have to begin with\\namassedthedemonstrateandpreparedit.10differentregressorsfromsupervised\\nlearning based on distinctive learning techniques have been executed to antici-\\npate precipitation‚Äôs most pertinent mode. This area depicted distinctive test er-\\nrands for the execution investigation and assessment and compared all calcula-\\ntions. Then, we have outlined the test setup utilized to execute the entire errand\\nand utilized 11 statistical assessment measurements for investigation execution.\\nAt long last, we have moreover compared with other works related to this issue\\nconcerning the finest form of our work.\\n4.1 Experiment Setup\\nwe have completed the complete computation in2google collab, a python reen-\\nactment environment given by Google. This environment comes with parallel', metadata={'source': 'data\\\\paper.pdf', 'page': 9}),\n",
       " Document(page_content='At long last, we have moreover compared with other works related to this issue\\nconcerning the finest form of our work.\\n4.1 Experiment Setup\\nwe have completed the complete computation in2google collab, a python reen-\\nactment environment given by Google. This environment comes with parallel\\ncomputation facilities for quick execution. We have utilized the foremost well-\\nknownlibrariestocreatesimpleandexpressiveinformationstructuresthatwork\\nwell and instinctively quickly. At long last, sklearn library contains specialized\\nmachine learning and statistical modeling instruments, counting classification,\\nregression,andclusteringcalculationsformodeling.Wehaveutilizedamachine\\nlearning system named3sci-kit learn to implement the regression algorithm.\\nAt long last, we utilized4matplotlib and5seaborn for information visualization,\\ngraphical representation, additionally for information investigation.\\n4.2 Statistical measurement\\nR2score:TheR2scorecouldbeaverycriticalmetricthat‚Äôsutilizedtoassessthe', metadata={'source': 'data\\\\paper.pdf', 'page': 9}),\n",
       " Document(page_content='At long last, we utilized4matplotlib and5seaborn for information visualization,\\ngraphical representation, additionally for information investigation.\\n4.2 Statistical measurement\\nR2score:TheR2scorecouldbeaverycriticalmetricthat‚Äôsutilizedtoassessthe\\nperformance of a regression based machine learning model. It is articulated as\\nR squared and is additionally known as the coefficient of assurance. It works by\\nmeasuring the sum of variance within the expectations clarified by the dataset.\\nBasicallyput,itisthecontrastbetweenthetestswithinthedatasetandtheexpec-\\ntations made by the demonstrate. As we can see from all models Random Forest\\nregressor achieves the best r2 score which is 0.869904217. The second and third\\n2https://colab.research.google.com/\\n3https://scikit-learn.org\\n4https://matplotlib.org\\n5https://seaborn.pydata.org\\n88', metadata={'source': 'data\\\\paper.pdf', 'page': 9}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\npositions are achieved by GradientBoostingRegressor and XGBoost which are\\n0.863496747 and 0.863215393. The condition is shown underneath in condition 1:\\nùëÖ2= 1 ‚àí‚àëùëõ\\nùëñ=1(ÃÇ ùë¶ùëñ‚àí ùë¶ùëñ)2\\n‚àëùëõ\\nùëñ=1(ùë¶ùëñ‚àí ÃÑ ùë¶ùëñ)2(7.1)\\nMean absolute error: If we consider with respect to error rate then first comes\\nto mean absolute error. In measurements, mean absolute error may be a degree\\nof blunders between combined perceptions communicating the same wonder.\\nMean Absolute Error (MAE) is another loss function utilized for relapse models.\\nMAE is the entirety of outright contrasts between our target and anticipated fac-\\ntors. So it measures the normal greatness of errors in a set of forecasts, without\\nconsidering their bearings. Random Forest regressor gets the least mean abso-\\nlute error rate which is 0.194459262 compare to others. The declaration of the F1\\nscore is displayed in equation 2 :\\nùëÄùê¥ùê∏ =1\\nùëõùëõ\\n‚àë\\nùëñ=1||ùëåùëñ‚àíÃÇùëåùëñ|| (7.2)', metadata={'source': 'data\\\\paper.pdf', 'page': 10}),\n",
       " Document(page_content='considering their bearings. Random Forest regressor gets the least mean abso-\\nlute error rate which is 0.194459262 compare to others. The declaration of the F1\\nscore is displayed in equation 2 :\\nùëÄùê¥ùê∏ =1\\nùëõùëõ\\n‚àë\\nùëñ=1||ùëåùëñ‚àíÃÇùëåùëñ|| (7.2)\\nMean squared error: If we consider with respect to mean squared error, The\\nmean squared error (MSE) tells how near a relapse line is to a set of focuses. It\\ndoes this by taking the separations from the focuses to the relapse line these sep-\\narationsaretheerrorsandsquaringthem,wecallItmeansquarederror.Fromall\\nthe models Random forest achieves a minimum mean squared error 0.126358647.\\nThe articulation is shown beneath in 3 :\\nùëÄùëÜùê∏ =1\\nùëõùëõ\\n‚àë\\nùëñ=1(ùëåùëñ‚àíÃÇùëåùëñ)2(7.3)\\nRoot mean squared error: Now if we consider the root mean squared error,\\nRoot Mean Square Error (RMSE) means the standard deviation of the residuals\\nwhich is prediction error. Residuals are a degree of how distant from the relapse\\nline information focuses are RMSE could be a degree of how to spread out these', metadata={'source': 'data\\\\paper.pdf', 'page': 10}),\n",
       " Document(page_content='Root Mean Square Error (RMSE) means the standard deviation of the residuals\\nwhich is prediction error. Residuals are a degree of how distant from the relapse\\nline information focuses are RMSE could be a degree of how to spread out these\\nresiduals are. Here root mean squared error of Random Forest is 0.355469615\\nwhich is less compare to others. The verbalization is shown in 4:\\nùëÖùëÄùëÜùê∏ =\\n‚àöùëõ\\n‚àë\\nùëñ=1(ÃÇ ùë¶ùëñ‚àí ùë¶ùëñ)2\\nùëõ(7.4)\\n89', metadata={'source': 'data\\\\paper.pdf', 'page': 10}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\nBy all the statistical performance analysis we can see Random forest is the\\nefficient regressor model and performing well in this use case.\\n4.3 Result & performance analysis\\nTable 2:Performance Metrics of different regressors\\nModel Name r2 score MAE MSE RMSE\\nRandom Forest 0.869904217 0.19445926 0.126358647 0.355469615\\nDecision Tree 0.742284572 0.21508858 0.250312287 0.500312189\\nLinear Regression 0.837495137 0.22694578 0.157836744 0.397286728\\nKNN Regressor 0.401557082 0.48855924 0.581252029 0.762398865\\nAdaBoost Regressor 0.786451397 0.37659111 0.207414199 0.455427491\\nGradient Boosting Regressor 0.863496747 0.20372662 0.132582057 0.364118191\\nXGBoost 0.863215393 0.20367076 0.132855329 0.364493249\\nRidge Regression 0.837495234 0.157836649 0.132855329 0.397286608\\nLasso Regression -5.91E-05 0.83158029 0.971331339 0.985561434\\nSVM 0.841801 0.203451 0.130951 0.345151', metadata={'source': 'data\\\\paper.pdf', 'page': 11}),\n",
       " Document(page_content='Gradient Boosting Regressor 0.863496747 0.20372662 0.132582057 0.364118191\\nXGBoost 0.863215393 0.20367076 0.132855329 0.364493249\\nRidge Regression 0.837495234 0.157836649 0.132855329 0.397286608\\nLasso Regression -5.91E-05 0.83158029 0.971331339 0.985561434\\nSVM 0.841801 0.203451 0.130951 0.345151\\nFrom Table 2, we showed statistical results and comparisons among all ma-\\nchine learning regressors.For better analysis, we choose some statistical pro-\\ncedures for numerical result computing such as r2 score, mean absolute error\\n(MAE), mean square error (MSE), root mean square error (RMSE). After devel-\\noping the models and testing all regressors, We can see that the Random For-\\nest has predicted the best accuracy of 0.869904217 among all others, and the\\nmean absolute error is 0.194459262 which is the lowest, mean squared error is\\n0.126358647 and the root mean squared error is 0.355469615. Considering all er-\\nrors and accuracy, it took the best place. Secondly, the gradient boosting regres-', metadata={'source': 'data\\\\paper.pdf', 'page': 11}),\n",
       " Document(page_content='mean absolute error is 0.194459262 which is the lowest, mean squared error is\\n0.126358647 and the root mean squared error is 0.355469615. Considering all er-\\nrors and accuracy, it took the best place. Secondly, the gradient boosting regres-\\nsor has gained better accuracy with the second place which is 0.863496747 with\\nthe mean absolute error is 0.203726623, mean squared error is 0.132582057 and\\nthe root mean squared error is 0.364118191. Thirdly, the XGBoost regressor has\\nacquired better accuracy, which is 0.863215393, along with the mean absolute er-\\n90', metadata={'source': 'data\\\\paper.pdf', 'page': 11}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\nroris0.203670766,meansquarederroris0.132855329andtherootmeansquared\\nerror is 0.364493249. Also, from the section on linear algorithms, we can figure\\nout that Linear Regression and Ridge Regression showed the almost same accu-\\nracy and so on. So in this analysis, we can although Random forest and Gradient\\nBoosting Regressor have acquired almost the same Accuracy but if we consider\\nthe evaluation metrics of then so, Random forest has a low error rate compare\\nto Gradient Boosting. So, here we have considered the Random forest approach.\\nOverall all of regressors showed a standard and acceptable performance.\\nThe bar chart is a graph for representing all regressors algorithms with Sta-\\ntistical measurement. The bar can be vertically or horizontally. Here is the bar\\ngraph of our selective algorithms, down below.\\nFigure 4 :Selective algorithms\\n5 Conclusion\\nIn this work, we have presented an initial attempt to determine how much rain', metadata={'source': 'data\\\\paper.pdf', 'page': 12}),\n",
       " Document(page_content='tistical measurement. The bar can be vertically or horizontally. Here is the bar\\ngraph of our selective algorithms, down below.\\nFigure 4 :Selective algorithms\\n5 Conclusion\\nIn this work, we have presented an initial attempt to determine how much rain\\nwill come when it‚Äôs raining time. In the data collection phase, we adopted real\\ndata from Australia from the Kaggle platform. The primary purpose of this task\\nistofindoutthebestregressiontechniqueforthepredictionofrain.Forthisrea-\\nson,wehaveusedavarietyofregressionanalysistechniquesthatcanbeutilized\\nfor predicting the quantity of rainfall so that anyone can use the best predictive\\nmodel in real-life applications. To perform this task, we selected five significant\\n91', metadata={'source': 'data\\\\paper.pdf', 'page': 12}),\n",
       " Document(page_content='steps, these are data collection, data prepossessing, training model using regres-\\nsion analysis techniques, and performance analysis. In pre-processing part, we\\nhave described cleaning data, Missing value check, EDA, Handling outliers, Fea-\\nture selection, Feature scaling respectively. Besides, we used ten supervised re-\\ngressors (machine learning models) for predicting rainfall. Among all models\\nthe are gives good accuracy in our predicting regression. Here, in the figure 4\\nthe graphical performance including compassion among all trained models has\\nbeen depicted\\n6 Future Work\\nInfuturework,wewillfocusonthereal-lifeapplicationofrainfallprediction,so\\nthat anyone especially farmer can use it easily and forecast the weather of rain.\\nAlso, we have plan to use the neural network based deep hybrid approaches\\nto improve the performance. Undoubtedly, we have plans to evaluate the other\\ncountry‚Äôs data for forecasting the rain.', metadata={'source': 'data\\\\paper.pdf', 'page': 13}),\n",
       " Document(page_content='Bibliography\\n[1]Ortiz-Garc√≠a, E. G., S. Salcedo-Sanz, and C. Casanova-Mateo. Accurate pre-\\ncipitation prediction with support vector classifiers: A study including\\nnovel predictive variables and observational data. Atmospheric research,\\n139:128‚Äì136, 2014.\\n[2]IanTyrrell. River Dreams: The people and landscape of the Cooks River . New-\\nSouth, 2018.\\n[3]John C Rodda. The rainfall measurement problem. IAHS Publication No ,\\n78:215‚Äì231, 1967.\\n[4]Gujanatti Rudrappa, Nataraj Vijapur, Rajesh Pattar, Ravi Rathod, Rashmi\\nKulkarni,VuduSreeChandana,andSateeshNHosmane. Machinelearning\\nmodels applied for rainfall prediction. REVISTA GEINTEC-GESTAO INOVA-\\nCAO E TECNOLOGIAS , 11(3):179‚Äì187, 2021.\\n[5]Zhi-liang Wang and Hui-hua Sheng. Rainfall prediction using generalized\\nregression neural network: case study zhengzhou. In International confer-\\nence on computational and information sciences ,pages1265‚Äì1268.IEEE,2010.\\n[6]Nikhil Sethi and Kanwal Garg. Exploiting data mining technique for rain-', metadata={'source': 'data\\\\paper.pdf', 'page': 14}),\n",
       " Document(page_content='[5]Zhi-liang Wang and Hui-hua Sheng. Rainfall prediction using generalized\\nregression neural network: case study zhengzhou. In International confer-\\nence on computational and information sciences ,pages1265‚Äì1268.IEEE,2010.\\n[6]Nikhil Sethi and Kanwal Garg. Exploiting data mining technique for rain-\\nfall prediction. International Journal of Computer Science and Information\\nTechnologies , 5(3):3982‚Äì3984, 2014.\\n[7]Sunyoung Lee, Sungzoon Cho, and Patrick M Wong. Rainfall prediction\\nusing artificial neural networks. journal of geographic information and De-\\ncision Analysis , 2(2):233‚Äì242, 1998.\\n[8]Adil M Bagirov, Arshad Mahmood, and Andrew Barton. Prediction of\\nmonthly rainfall in victoria, australia: Clusterwise linear regression ap-\\nproach. Atmospheric research , 188:20‚Äì29, 2017.\\n[9]Mohammed Moulana, Kolapalli Roshitha, Golla Niharika, and Maturi Siva\\nSai. Prediction of rainfall using machine learning techniques. International', metadata={'source': 'data\\\\paper.pdf', 'page': 14}),\n",
       " Document(page_content='monthly rainfall in victoria, australia: Clusterwise linear regression ap-\\nproach. Atmospheric research , 188:20‚Äì29, 2017.\\n[9]Mohammed Moulana, Kolapalli Roshitha, Golla Niharika, and Maturi Siva\\nSai. Prediction of rainfall using machine learning techniques. International\\nJournal of Scientific & Technology Research , 9:3236‚Äì3240, 2020.\\n2021. In Raju Pal & Praveen K. Shukla (eds.), SCRS Conference Proceedings\\non Intelligent Systems , 79‚Äì95. Computing & Intelligent Systems, SCRS, India.\\nhttps://doi.org/10.52458/978-93-91842-08-6-7', metadata={'source': 'data\\\\paper.pdf', 'page': 14}),\n",
       " Document(page_content='Nusrat Jahan Prottasha , Md. Jashim Uddin , Md. Kowsher , Rokeya Khatun\\nShorna , Niaz Al Murshed & Boktiar Ahmed Bappy\\n[10]P Asha, A Jesudoss, S Prince Mary, KV Sai Sandeep, and K Harsha Vardhan.\\nAn efficient hybrid machine learning classifier for rainfall prediction. In\\nJournal of Physics: Conference Series , volume 1770, page 012012, 2021.\\n[11]S Sakthivel et al. Effective procedure to predict rainfall conditions using\\nhybrid machine learning strategies. Turkish Journal of Computer and Math-\\nematics Education (TURCOMAT) , 12(6):209‚Äì216, 2021.\\n[12]Diwakar Naidu, Babita Majhi, and Surendra Kumar Chandniha. Develop-\\nment of rainfall prediction models using machine learning approaches for\\ndifferent agro-climatic zones. In Handbook of Research on Automated Fea-\\nture Engineering and Advanced Applications in Data Science , pages 72‚Äì94.\\nIGI Global, 2021.\\n[13]Tuan Vu Dinh, Hieu Nguyen, Xuan-Linh Tran, and Nhat-Duc Hoang. Pre-', metadata={'source': 'data\\\\paper.pdf', 'page': 15}),\n",
       " Document(page_content='different agro-climatic zones. In Handbook of Research on Automated Fea-\\nture Engineering and Advanced Applications in Data Science , pages 72‚Äì94.\\nIGI Global, 2021.\\n[13]Tuan Vu Dinh, Hieu Nguyen, Xuan-Linh Tran, and Nhat-Duc Hoang. Pre-\\ndicting rainfall-induced soil erosion based on a hybridization of adaptive\\ndifferentialevolutionandsupportvectormachineclassification. Mathemat-\\nical Problems in Engineering , 2021.\\n[14]RMalathi andM Manimekalai. Ant colony‚Äìinformation gainbased feature\\nselection method for weather dataset. Annals of the Romanian Society for\\nCell Biology , pages 3838‚Äì3850, 2021.\\n[15]NorSamsiahSani,IsraaShlash,MohammedHassan,AbdulHadi,andMohd\\nAliff.Enhancingmalaysiarainfallpredictionusingclassificationtechniques.\\nJ. Appl. Environ. Biol. Sci , 7(2S):20‚Äì29, 2017.\\n[16]David Ahijevych, James O Pinto, John K Williams, and Matthias Steiner.\\nProbabilistic forecasts of mesoscale convective system initiation using the', metadata={'source': 'data\\\\paper.pdf', 'page': 15}),\n",
       " Document(page_content='Aliff.Enhancingmalaysiarainfallpredictionusingclassificationtechniques.\\nJ. Appl. Environ. Biol. Sci , 7(2S):20‚Äì29, 2017.\\n[16]David Ahijevych, James O Pinto, John K Williams, and Matthias Steiner.\\nProbabilistic forecasts of mesoscale convective system initiation using the\\nrandom forest data mining technique. Weather and Forecasting , 31(2):581‚Äì\\n599, 2016.\\n[17]JohnTAllen. Climatechangeandseverethunderstorms. In Oxford research\\nencyclopedia of climate science . 2018.\\n[18]Harold E Brooks. Severe thunderstorms and climate change. Atmospheric\\nResearch , 123:129‚Äì138, 2013.\\n[19]Pierre Gentine, Mike Pritchard, Stephan Rasp, Gael Reinaudi, and Galen\\nYacalis. Could machine learning break the convection parameterization\\ndeadlock? Geophysical Research Letters , 45(11):5742‚Äì5751, 2018.\\n94', metadata={'source': 'data\\\\paper.pdf', 'page': 15}),\n",
       " Document(page_content='SCRS Conference Proceedings on Intelligent Systems (2021)\\n[20]Michael J Mcphaden, Gary Meyers, K Ando, Y Masumoto, VSN Murty,\\nMRavichandran,FSyamsudin,J√©r√¥meVialard,LianboYu,andWYu. Rama:\\nthe research moored array for african‚Äìasian‚Äìaustralian monsoon analysis\\nand prediction. Bulletin of the American Meteorological Society , 90(4):459‚Äì\\n480, 2009.\\n[21]Peter BR Hazell. The appropriate role of agricultural insurance in develop-\\ning countries. Journal of International Development , 4(6):567‚Äì581, 1992.\\n[22]Peter P Mollinga, Ruth S Meinzen-Dick, and Douglas J Merrey. Politics,\\npluralityandproblemsheds:Astrategicapproachforreformofagricultural\\nwater resources management. Development Policy Review , 25(6):699‚Äì719,\\n2007.\\n[23]Chirag Shah, Chathra Hendahewa, and Roberto Gonz√°lez-Ib√°√±ez. Rain or\\nshine? forecasting search process performance in exploratory search tasks.\\nJournal of the Association for Information Science and Technology ,67(7):1607‚Äì\\n1623, 2016.', metadata={'source': 'data\\\\paper.pdf', 'page': 16}),\n",
       " Document(page_content='2007.\\n[23]Chirag Shah, Chathra Hendahewa, and Roberto Gonz√°lez-Ib√°√±ez. Rain or\\nshine? forecasting search process performance in exploratory search tasks.\\nJournal of the Association for Information Science and Technology ,67(7):1607‚Äì\\n1623, 2016.\\n[24]GarethJames,DanielaWitten,TrevorHastie,andRobertTibshirani. Linear\\nmodelselectionandregularization. In An introduction to statistical learning ,\\npages 225‚Äì288. Springer, 2021.\\n[25]Raksha Agarwal and Niladri Chatterjee. Langresearchlab_nc at cmcl2021\\nshared task: Predicting gaze behaviour using linguistic features and tree\\nregressors. In Proceedings of the Workshop on Cognitive Modeling and Com-\\nputational Linguistics , pages 79‚Äì84, 2021.\\n[26]S Ben√≠tez-Pe√±a, E Carrizosa, V Guerrero, MD Jim√©nez-Gamero, B Mart√≠n-\\nBarrag√°n, and C Molero-R√≠o. On sparse ensemble methods. 2021.\\n[27]KimdeBie,AnaLucic,andHindaHaned. Totrustornottotrustaregressor:\\nEstimating and explaining trustworthiness of regression predictions. arXiv', metadata={'source': 'data\\\\paper.pdf', 'page': 16}),\n",
       " Document(page_content='[26]S Ben√≠tez-Pe√±a, E Carrizosa, V Guerrero, MD Jim√©nez-Gamero, B Mart√≠n-\\nBarrag√°n, and C Molero-R√≠o. On sparse ensemble methods. 2021.\\n[27]KimdeBie,AnaLucic,andHindaHaned. Totrustornottotrustaregressor:\\nEstimating and explaining trustworthiness of regression predictions. arXiv\\npreprint arXiv:2104.06982 , 2021.\\n[28]MauricioGonz√°lez-Palacio,LinaSep√∫lveda-Cano,andRonalMontoya. Sim-\\nplified path loss lognormal shadow fading model versus a support vector\\nmachine-based regressor comparison for determining reception powers in\\nwlan networks. In International Conference on Information Technology &\\nSystems, pages 431‚Äì441. Springer, 2021.\\n95', metadata={'source': 'data\\\\paper.pdf', 'page': 16})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=300\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
